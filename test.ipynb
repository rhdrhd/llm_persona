{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. import dataset and load sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['personality', 'candidates', 'history', 'conv_id', 'utterance_idx'],\n",
      "        num_rows: 14\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['personality', 'candidates', 'history', 'conv_id', 'utterance_idx'],\n",
      "        num_rows: 15\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"bavard/personachat_truecased\", \"sample\")\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'personality': ['I like to remodel homes.', 'I like to go hunting.', 'I like to shoot a bow.', 'My favorite holiday is halloween.'], 'candidates': ['My mom was single with 3 boys, so we never left the projects.', 'I try to wear all black every day. It makes me feel comfortable.', 'Well nursing stresses you out so I wish luck with sister.', 'Yeah just want to pick up Nba nfl getting old.', 'I really like Celine Dion. What about you?', 'No. I live near farms.', \"I wish I had a daughter, I'm a boy mom. They're beautiful boys though still lucky.\", 'Yeah when I get bored I play gone with the wind my favorite movie.', \"Hi how are you? I'm eating dinner with my hubby and 2 kids.\", 'Were you married to your high school sweetheart? I was.', 'That is great to hear! Are you a competitive rider?', \"Hi, I'm doing ok. I'm a banker. How about you?\", \"I'm 5 years old.\", 'Hi there. How are you today?', 'I totally understand how stressful that can be.', 'Yeah sometimes you do not know what you are actually watching.', 'Mother taught me to cook! We are looking for an exterminator.', 'I enjoy romantic movie. What is your favorite season? Mine is summer.', 'Editing photos takes a lot of work.', 'You must be very fast. Hunting is one of my favorite hobbies.'], 'history': [\"Hi, how are you doing? I'm getting ready to do some cheetah chasing to stay in shape.\"], 'conv_id': 0, 'utterance_idx': 0}\n"
     ]
    }
   ],
   "source": [
    "print(dataset[\"train\"][0])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Define history conversation, response, and persona"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "#select conversation with certain id\n",
    "conv_id = 6\n",
    "dataset = load_dataset(\"bavard/personachat_truecased\", \"full\")\n",
    "dialog = dataset.filter(lambda example:example['conv_id']== conv_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User Persona:  ['I have a boxer dog.', 'I like baths.', 'I like to listen to music.', 'My father lives in China.'] \n",
      "\n",
      "History Conversation:  [\"Rock on, I'm listening to my favorite band guns and roses.\", 'No kidding? I was just listening to the same thing while taking a bath.', 'Of course. I love to listen to rock.', 'Man my boxer just peed on the carpet!', \"Well I'm into black everything. So at least it wouldn't show on my black carpet.\", 'Ll. I love black too! Guess I was playing my music too loud.', \"I've a black car, purse, wear all black.\", 'Maybe I can borrow something as I am packing to visit my dad in China.', 'Wow, does he live there or work?', 'Live. Moved there about ten years ago for a computer tech job.', 'Have you visited him there before?', 'Once. You cannot even throw a gum wrapper or you can get arrested.', 'Sounds a bit scary. I ve never been.'] \n",
      "\n",
      "Response:  Well not too much crime there, but a lot of people.\n"
     ]
    }
   ],
   "source": [
    "#select persona \n",
    "persona = dialog['train']['personality'][-1]\n",
    "print(\"User Persona: \", persona, \"\\n\")\n",
    "\n",
    "#select the history conversation\n",
    "history_convo = dialog['train']['history'][-1]\n",
    "print(\"History Conversation: \", history_convo, \"\\n\")\n",
    "\n",
    "#select the response\n",
    "usr_response = dialog['train']['candidates'][-1][-1]\n",
    "print(\"Response: \",usr_response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Preprocess the text to make it suitable for prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dialogue history: \n",
      "Bot: Rock on, I'm listening to my favorite band guns and roses.\n",
      "User: No kidding? I was just listening to the same thing while taking a bath.\n",
      "Bot: Of course. I love to listen to rock.\n",
      "User: Man my boxer just peed on the carpet!\n",
      "Bot: Well I'm into black everything. So at least it wouldn't show on my black carpet.\n",
      "User: Ll. I love black too! Guess I was playing my music too loud.\n",
      "Bot: I've a black car, purse, wear all black.\n",
      "User: Maybe I can borrow something as I am packing to visit my dad in China.\n",
      "Bot: Wow, does he live there or work?\n",
      "User: Live. Moved there about ten years ago for a computer tech job.\n",
      "Bot: Have you visited him there before?\n",
      "User: Once. You cannot even throw a gum wrapper or you can get arrested.\n",
      "Bot: Sounds a bit scary. I ve never been.\n",
      "\n",
      "Response\n",
      "User:\n"
     ]
    }
   ],
   "source": [
    "#preprocess the persona\n",
    "persona_processed = \"User Persona: \\n\"\n",
    "for sen in persona:\n",
    "    persona_processed += sen + \"\\n\"\n",
    "\n",
    "#preprocess the history conversation\n",
    "history_convo_processed = \"Dialogue history: \\n\"\n",
    "\n",
    "#concat all history except the last utter from the bot\n",
    "for i in range(0,len(history_convo)-1,2):\n",
    "    bot_uttr = \"Bot: \" + history_convo[i]\n",
    "    user_uttr = \"User: \" + history_convo[i+1]\n",
    "    full_uttr = bot_uttr + \"\\n\" + user_uttr + \"\\n\"\n",
    "    history_convo_processed += full_uttr\n",
    "\n",
    "#add the last utter from bot\n",
    "history_convo_processed += \"Bot: \" + history_convo[-1] + \"\\n\"\n",
    "\n",
    "#concat prompt\n",
    "prompt = persona_processed + \"\\n\" + history_convo_processed + \"\\nResponse\\nUser:\"\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "\n",
    "os.environ['OPENAI_API_KEY'] = 'sk-proj-zgpS1dH01GeQiQzhICjNT3BlbkFJq8oC1i8atPEckRUxahYK'\n",
    "client = OpenAI()\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo\",  # Specify the model\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"According to the history conversation provided, pretend to speak like a human user and generate a response.\"},\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ],\n",
    "    temperature=0,  # Adjust based on how creative you want the AI to be\n",
    "    max_tokens=150, # Adjust based on how long you expect responses to be\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"id\": \"chatcmpl-9ikTtLOpcukHYtnHPaUuqlCtz9enM\",\n",
      "    \"choices\": [\n",
      "        {\n",
      "            \"finish_reason\": \"stop\",\n",
      "            \"index\": 0,\n",
      "            \"logprobs\": null,\n",
      "            \"message\": {\n",
      "                \"content\": \"China is definitely a unique experience with its strict rules. It's important to be mindful of the cultural differences when visiting.\",\n",
      "                \"role\": \"assistant\",\n",
      "                \"function_call\": null,\n",
      "                \"tool_calls\": null\n",
      "            }\n",
      "        }\n",
      "    ],\n",
      "    \"created\": 1720452133,\n",
      "    \"model\": \"gpt-3.5-turbo-0125\",\n",
      "    \"object\": \"chat.completion\",\n",
      "    \"service_tier\": null,\n",
      "    \"system_fingerprint\": null,\n",
      "    \"usage\": {\n",
      "        \"completion_tokens\": 24,\n",
      "        \"prompt_tokens\": 233,\n",
      "        \"total_tokens\": 257\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "print(json.dumps(json.loads(response.model_dump_json()), indent=4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "China is definitely a unique experience with its strict rules. It's important to be mindful of the cultural differences when visiting.\n",
      "Well not too much crime there, but a lot of people.\n"
     ]
    }
   ],
   "source": [
    "response_chatgpt = response.choices[0].message.content\n",
    "\n",
    "print(response_chatgpt)\n",
    "\n",
    "print(usr_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import random\n",
    "\n",
    "def get_random_conv_id(pool, num_conv):\n",
    "    random_values = random.sample(pool, num_conv)\n",
    "\n",
    "    for value in random_values:\n",
    "        pool.remove(value)\n",
    "    \n",
    "    return random_values\n",
    "\n",
    "def extract_format_data(dataset, conv_id):\n",
    "    dialog = dataset.filter(lambda example:example['conv_id']== conv_id)\n",
    "\n",
    "    persona = dialog['train']['personality'][-1]\n",
    "    history_convo = dialog['train']['history'][-1]\n",
    "    usr_response = dialog['train']['candidates'][-1][-1]\n",
    "\n",
    "    #preprocess the persona\n",
    "    persona_processed = \"User Persona: \\n\"\n",
    "    for sen in persona:\n",
    "        persona_processed += sen + \"\\n\"\n",
    "\n",
    "    #preprocess the history conversation\n",
    "    history_convo_processed = \"Dialogue history: \\n\"\n",
    "\n",
    "    #concat all history except the last utter from the bot\n",
    "    for i in range(0,len(history_convo)-1,2):\n",
    "        bot_uttr = \"Bot: \" + history_convo[i]\n",
    "        user_uttr = \"User: \" + history_convo[i+1]\n",
    "        full_uttr = bot_uttr + \"\\n\" + user_uttr + \"\\n\"\n",
    "        history_convo_processed += full_uttr\n",
    "\n",
    "    #add the last utter from bot\n",
    "    history_convo_processed += \"Bot: \" + history_convo[-1] + \"\\n\"\n",
    "\n",
    "    #preprocess the user response\n",
    "    usr_response_processed = \"Response\\nUser: \" + usr_response + \"\\n\"\n",
    "    return [persona_processed, history_convo_processed, usr_response_processed]\n",
    "\n",
    "def create_example(dataset, conv_id, implicit=False):\n",
    "    materials = extract_format_data(dataset, conv_id)\n",
    "    if implicit:\n",
    "        return materials[1] + materials[2]\n",
    "    else:\n",
    "        return materials[0] + materials[1] + materials[2]\n",
    "    return example\n",
    "\n",
    "def create_few_shot_examples(dataset, conv_id, few_shot_no, implicit=False):\n",
    "    max_conv_id = dataset['train']['conv_id'][-1]\n",
    "    pool = [num for num in range(1, max_conv_id + 1) if num != conv_id]\n",
    "    random_conv_ids = get_random_conv_id(pool, few_shot_no)\n",
    "\n",
    "    few_shot_examples = \"\"\n",
    "    for index, conv_id in enumerate(random_conv_ids):\n",
    "        few_shot_examples += f\"Demo {index}:\\n\" + create_example(dataset, conv_id, implicit) + \"\\n\"\n",
    "    return few_shot_examples\n",
    "\n",
    "\n",
    "def construct_prompt(dataset, conv_id, prompt_type, few_shot_no):\n",
    "    max_conv_id = dataset['train']['conv_id'][-1]\n",
    "    materials = extract_format_data(dataset, conv_id)\n",
    "    system_prompt = \"\"\n",
    "    user_prompt = \"\"\n",
    "    few_shot_examples = \"\"\n",
    "\n",
    "    if prompt_type == \"context_only\":\n",
    "        user_prompt += materials[1] + \"User:\"\n",
    "\n",
    "    if prompt_type == \"task_prompt_context_implicit\":\n",
    "        system_prompt += \"According to the history conversation provided, pretend to speak like a human user and generate a response.\"\n",
    "        user_prompt += materials[1] + \"User:\"\n",
    "\n",
    "    if prompt_type == \"task_prompt_context_explicit\":\n",
    "        system_prompt += \"According to the persona and history conversation provided, pretend to speak like a human user and generate a response.\"\n",
    "        user_prompt += materials[0]+ materials[1] + \"User:\"\n",
    "\n",
    "    if prompt_type == \"few_shot_implicit\":\n",
    "        system_prompt += \"According to the few shot demos provided, pretend to speak like User and generate a response matching the context.\"\n",
    "\n",
    "        few_shot_examples = create_few_shot_examples(dataset, conv_id, few_shot_no,implicit=True)\n",
    "        user_prompt += few_shot_examples + materials[1]+ \"User:\"\n",
    "\n",
    "    return system_prompt, user_prompt, materials[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|██████████| 131438/131438 [00:02<00:00, 52279.80 examples/s]\n",
      "Filter: 100%|██████████| 7801/7801 [00:00<00:00, 52893.94 examples/s]\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(\"bavard/personachat_truecased\", \"full\")\n",
    "system_p, user_p, target_response = construct_prompt(dataset, 6, \"few_shot_implicit\", 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'According to the few shot demos provided, pretend to speak like User and generate a response matching the context.'"
      ]
     },
     "execution_count": 278,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "system_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "\n",
    "os.environ['OPENAI_API_KEY'] = 'sk-proj-zgpS1dH01GeQiQzhICjNT3BlbkFJq8oC1i8atPEckRUxahYK'\n",
    "client = OpenAI()\n",
    "\n",
    "def prompt_chatgpt(prompt_type, conv_id, few_shot_no=0):\n",
    "    dataset = load_dataset(\"bavard/personachat_truecased\", \"full\")\n",
    "    system_prompt, user_prompt, target_response= construct_prompt(dataset, conv_id, prompt_type, few_shot_no)\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo-16k\",  # Specify the model\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ],\n",
    "        temperature=0.9,  # Adjust based on how creative you want the AI to be\n",
    "        max_tokens=15, # Adjust based on how long you expect responses to be\n",
    "    )\n",
    "    return response.choices[0].message.content, user_prompt, target_response\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "response, user_prompt, target_repsonse= prompt_chatgpt(\"task_prompt_context_explicit\", 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User Persona: \n",
      "I have a boxer dog.\n",
      "I like baths.\n",
      "I like to listen to music.\n",
      "My father lives in China.\n",
      "Dialogue history: \n",
      "Bot: Rock on, I'm listening to my favorite band guns and roses.\n",
      "User: No kidding? I was just listening to the same thing while taking a bath.\n",
      "Bot: Of course. I love to listen to rock.\n",
      "User: Man my boxer just peed on the carpet!\n",
      "Bot: Well I'm into black everything. So at least it wouldn't show on my black carpet.\n",
      "User: Ll. I love black too! Guess I was playing my music too loud.\n",
      "Bot: I've a black car, purse, wear all black.\n",
      "User: Maybe I can borrow something as I am packing to visit my dad in China.\n",
      "Bot: Wow, does he live there or work?\n",
      "User: Live. Moved there about ten years ago for a computer tech job.\n",
      "Bot: Have you visited him there before?\n",
      "User: Once. You cannot even throw a gum wrapper or you can get arrested.\n",
      "Bot: Sounds a bit scary. I ve never been.\n",
      "User:\n",
      "It can be a bit intimidating at times, but overall it's a fascinating\n",
      "Response\n",
      "User: Well not too much crime there, but a lot of people.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(user_prompt)\n",
    "print(response)\n",
    "print(target_repsonse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"It can be a bit intimidating at times, but overall it's a fascinating\""
      ]
     },
     "execution_count": 286,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "131438\n"
     ]
    }
   ],
   "source": [
    "max_conv_id = len(dataset['train']['conv_id'])\n",
    "print(max_conv_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User Persona: \n",
      "I have a boxer dog.\n",
      "I like baths.\n",
      "I like to listen to music.\n",
      "My father lives in China.\n",
      "Dialogue history: \n",
      "Bot: Rock on, I'm listening to my favorite band guns and roses.\n",
      "User: No kidding? I was just listening to the same thing while taking a bath.\n",
      "Bot: Of course. I love to listen to rock.\n",
      "User: Man my boxer just peed on the carpet!\n",
      "Bot: Well I'm into black everything. So at least it wouldn't show on my black carpet.\n",
      "User: Ll. I love black too! Guess I was playing my music too loud.\n",
      "Bot: I've a black car, purse, wear all black.\n",
      "User: Maybe I can borrow something as I am packing to visit my dad in China.\n",
      "Bot: Wow, does he live there or work?\n",
      "User: Live. Moved there about ten years ago for a computer tech job.\n",
      "Bot: Have you visited him there before?\n",
      "User: Once. You cannot even throw a gum wrapper or you can get arrested.\n",
      "Bot: Sounds a bit scary. I ve never been.\n",
      "Response:\n",
      "User: Well not too much crime there, but a lot of people.\n"
     ]
    }
   ],
   "source": [
    "print(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rouge_score import rouge_scorer\n",
    "\n",
    "scorer = rouge_scorer.RougeScorer(['rouge1', 'rougeL'], use_stemmer=True)\n",
    "scores = scorer.score('The quick brown fox jumps over the lazy dog',\n",
    "                      'The quick brown dog jumps on the log.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7058823529411765"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores['rouge1'].fmeasure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/zarius/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt/PY3/english.pickle\u001b[0m\n\n  Searched in:\n    - '/Users/zarius/nltk_data'\n    - '/Users/zarius/miniconda3/envs/llm_persona/nltk_data'\n    - '/Users/zarius/miniconda3/envs/llm_persona/share/nltk_data'\n    - '/Users/zarius/miniconda3/envs/llm_persona/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - ''\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[252], line 29\u001b[0m\n\u001b[1;32m     26\u001b[0m persona \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mI love playing basketball and enjoy watching movies.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     27\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBasketball is my favorite sport. I also love watching movies in my free time.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 29\u001b[0m p_cover_score \u001b[38;5;241m=\u001b[39m \u001b[43mp_cover\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpersona\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mP-Cover Score: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mp_cover_score\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[252], line 10\u001b[0m, in \u001b[0;36mp_cover\u001b[0;34m(persona, response)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mp_cover\u001b[39m(persona, response):\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;66;03m# Tokenize and remove stop words\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m     persona_tokens \u001b[38;5;241m=\u001b[39m [word \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m \u001b[43mnltk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mword_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpersona\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlower\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m word\u001b[38;5;241m.\u001b[39misalnum() \u001b[38;5;129;01mand\u001b[39;00m word \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m stop_words]\n\u001b[1;32m     11\u001b[0m     response_tokens \u001b[38;5;241m=\u001b[39m [word \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m nltk\u001b[38;5;241m.\u001b[39mword_tokenize(response\u001b[38;5;241m.\u001b[39mlower()) \u001b[38;5;28;01mif\u001b[39;00m word\u001b[38;5;241m.\u001b[39misalnum() \u001b[38;5;129;01mand\u001b[39;00m word \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m stop_words]\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;66;03m# Calculate TF-IDF for the persona terms\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/llm_persona/lib/python3.9/site-packages/nltk/tokenize/__init__.py:129\u001b[0m, in \u001b[0;36mword_tokenize\u001b[0;34m(text, language, preserve_line)\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mword_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m, preserve_line\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m    115\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;124;03m    Return a tokenized copy of *text*,\u001b[39;00m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;124;03m    :type preserve_line: bool\u001b[39;00m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 129\u001b[0m     sentences \u001b[38;5;241m=\u001b[39m [text] \u001b[38;5;28;01mif\u001b[39;00m preserve_line \u001b[38;5;28;01melse\u001b[39;00m \u001b[43msent_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[1;32m    131\u001b[0m         token \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m sentences \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m _treebank_word_tokenizer\u001b[38;5;241m.\u001b[39mtokenize(sent)\n\u001b[1;32m    132\u001b[0m     ]\n",
      "File \u001b[0;32m~/miniconda3/envs/llm_persona/lib/python3.9/site-packages/nltk/tokenize/__init__.py:106\u001b[0m, in \u001b[0;36msent_tokenize\u001b[0;34m(text, language)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msent_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     97\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;124;03m    Return a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;124;03m    :param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 106\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtokenizers/punkt/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mlanguage\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.pickle\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    107\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer\u001b[38;5;241m.\u001b[39mtokenize(text)\n",
      "File \u001b[0;32m~/miniconda3/envs/llm_persona/lib/python3.9/site-packages/nltk/data.py:750\u001b[0m, in \u001b[0;36mload\u001b[0;34m(resource_url, format, cache, verbose, logic_parser, fstruct_reader, encoding)\u001b[0m\n\u001b[1;32m    747\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<<Loading \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresource_url\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m>>\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    749\u001b[0m \u001b[38;5;66;03m# Load the resource.\u001b[39;00m\n\u001b[0;32m--> 750\u001b[0m opened_resource \u001b[38;5;241m=\u001b[39m \u001b[43m_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresource_url\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    752\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mformat\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    753\u001b[0m     resource_val \u001b[38;5;241m=\u001b[39m opened_resource\u001b[38;5;241m.\u001b[39mread()\n",
      "File \u001b[0;32m~/miniconda3/envs/llm_persona/lib/python3.9/site-packages/nltk/data.py:876\u001b[0m, in \u001b[0;36m_open\u001b[0;34m(resource_url)\u001b[0m\n\u001b[1;32m    873\u001b[0m protocol, path_ \u001b[38;5;241m=\u001b[39m split_resource_url(resource_url)\n\u001b[1;32m    875\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m protocol \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m protocol\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnltk\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 876\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mopen()\n\u001b[1;32m    877\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m protocol\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfile\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    878\u001b[0m     \u001b[38;5;66;03m# urllib might not use mode='rb', so handle this one ourselves:\u001b[39;00m\n\u001b[1;32m    879\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m find(path_, [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m])\u001b[38;5;241m.\u001b[39mopen()\n",
      "File \u001b[0;32m~/miniconda3/envs/llm_persona/lib/python3.9/site-packages/nltk/data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    581\u001b[0m sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m70\u001b[39m\n\u001b[1;32m    582\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 583\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt/PY3/english.pickle\u001b[0m\n\n  Searched in:\n    - '/Users/zarius/nltk_data'\n    - '/Users/zarius/miniconda3/envs/llm_persona/nltk_data'\n    - '/Users/zarius/miniconda3/envs/llm_persona/share/nltk_data'\n    - '/Users/zarius/miniconda3/envs/llm_persona/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - ''\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def p_cover(persona, response):\n",
    "    # Tokenize and remove stop words\n",
    "    persona_tokens = [word for word in nltk.word_tokenize(persona.lower()) if word.isalnum() and word not in stop_words]\n",
    "    response_tokens = [word for word in nltk.word_tokenize(response.lower()) if word.isalnum() and word not in stop_words]\n",
    "    \n",
    "    # Calculate TF-IDF for the persona terms\n",
    "    vectorizer = TfidfVectorizer(vocabulary=set(persona_tokens))\n",
    "    tfidf_matrix = vectorizer.fit_transform([response])\n",
    "    \n",
    "    # Extract the TF-IDF scores for the persona terms in the response\n",
    "    tfidf_scores = tfidf_matrix.toarray()[0]\n",
    "    \n",
    "    # Calculate the P-Cover score\n",
    "    p_cover_score = sum(tfidf_scores) / len(persona_tokens)\n",
    "    \n",
    "    return p_cover_score\n",
    "\n",
    "# Example usage\n",
    "persona = \"I love playing basketball and enjoy watching movies.\"\n",
    "response = \"Basketball is my favorite sport. I also love watching movies in my free time.\"\n",
    "\n",
    "p_cover_score = p_cover(persona, response)\n",
    "print(f\"P-Cover Score: {p_cover_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from rouge_score import rouge_scorer\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "\n",
    "\n",
    "# implementation of P-Cover\n",
    "def calculate_p_cover(persona, response):\n",
    "    return 0\n",
    "\n",
    "# implementation of BLEU-1, BLEU-2, BLEU-3, and BLEU-4\n",
    "def calculate_bleu(reference_sentence, candidate_sentence):\n",
    "    reference = [reference_sentence.split()]\n",
    "    candidate = candidate_sentence.split()\n",
    "    \n",
    "    # Weights for BLEU-1, BLEU-2, BLEU-3, and BLEU-4\n",
    "    weights = [\n",
    "        (1.0, 0, 0, 0),        \n",
    "        (0.5, 0.5, 0, 0),        \n",
    "        (1.0 / 3, 1.0 / 3, 1.0 / 3, 0), \n",
    "        (0.25, 0.25, 0.25, 0.25) \n",
    "    ]\n",
    "    \n",
    "    # Use smoothing to handle cases with few n-grams\n",
    "    smoothing_function = SmoothingFunction()\n",
    "    scores = sentence_bleu(reference, candidate, weights, smoothing_function=smoothing_function.method1)\n",
    "    return scores\n",
    "\n",
    "# rouge1, rouge2, rougeL\n",
    "def calculate_rouge(reference_sentence, candidate_sentence):\n",
    "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "    scores = scorer.score(reference_sentence, candidate_sentence)\n",
    "    return scores\n",
    "\n",
    "def cosine_similarity_embeddings(sentence1, sentence2, model_name='all-MiniLM-L6-v2'):\n",
    "    model = SentenceTransformer(model_name)\n",
    "    embeddings = model.encode([sentence1, sentence2])\n",
    "    cos_sim = cosine_similarity([embeddings[0]], [embeddings[1]])[0][0]\n",
    "    return cos_sim\n",
    "\n",
    "def distinct_1(sentence):\n",
    "    words = sentence.split()\n",
    "    unique_unigrams = set(words)\n",
    "    total_unigrams = len(words)\n",
    "    if total_unigrams == 0:\n",
    "        return 0.0\n",
    "    distinct_1_score = len(unique_unigrams) / total_unigrams\n",
    "    return distinct_1_score\n",
    "\n",
    "def distinct_2(sentence):\n",
    "    words = sentence.split()\n",
    "    bigrams = list(nltk.bigrams(words))\n",
    "    unique_bigrams = set(bigrams)\n",
    "    total_bigrams = len(bigrams)\n",
    "    if total_bigrams == 0:\n",
    "        return 0.0\n",
    "    distinct_2_score = len(unique_bigrams) / total_bigrams\n",
    "    return distinct_2_score\n",
    "\n",
    "def calculate_metrics(reference_sentence, candidate_sentence):\n",
    "    bleu_score = calculate_bleu(reference_sentence, candidate_sentence)\n",
    "    rouge_scores = calculate_rouge(reference_sentence, candidate_sentence)\n",
    "    cosine_similarity = cosine_similarity_embeddings(reference_sentence, candidate_sentence)\n",
    "    distinct_1_score = distinct_1(candidate_sentence)\n",
    "    distinct_2_score = distinct_2(candidate_sentence)\n",
    "\n",
    "    return {\n",
    "        'BLEU-1': bleu_score[0],\n",
    "        'BLEU-2': bleu_score[1],\n",
    "        'BLEU-3': bleu_score[2],\n",
    "        'BLEU-4': bleu_score[3],\n",
    "        'ROUGE-1': rouge_scores['rouge1'].fmeasure,\n",
    "        'ROUGE-2': rouge_scores['rouge2'].fmeasure,\n",
    "        'ROUGE-L': rouge_scores['rougeL'].fmeasure,\n",
    "        'Cosine Similarity': cosine_similarity,\n",
    "        'Distinct-1': distinct_1_score,\n",
    "        'Distinct-2': distinct_2_score\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'BLEU-1': 0.8187307530779819,\n",
       " 'BLEU-2': 0.7090416310250969,\n",
       " 'BLEU-3': 0.6498270293573523,\n",
       " 'BLEU-4': 0.5789300674674098,\n",
       " 'ROUGE-1': 0.9090909090909091,\n",
       " 'ROUGE-2': 0.6666666666666665,\n",
       " 'ROUGE-L': 0.9090909090909091,\n",
       " 'Cosine Similarity': 0.9925266,\n",
       " 'Distinct-1': 1.0,\n",
       " 'Distinct-2': 1.0}"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reference_sentence = \"the cat is on the mat\"\n",
    "candidate_sentence = \"the cat is on mat\"\n",
    "calculate_metrics(reference_sentence, candidate_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_stories = 10\n",
    "prompts = [\"Once upon a time,\"] * num_stories\n",
    "stories = [\"\"] * len(prompts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['', '', '', '', '', '', '', '', '', '']"
      ]
     },
     "execution_count": 262,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User Prompt: Dialogue history: \n",
      "Bot: Hi, how are you doing today?\n",
      "User: I am spending time with my 4 sisters what are you up to.\n",
      "Bot: Wow, four sisters. Just watching game of thrones.\n",
      "User: That is a good show I watch that while drinking iced tea.\n",
      "Bot: I agree. What do you do for a living?\n",
      "User: I'm a researcher I'm researching the fact that mermaids are real.\n",
      "Bot: Interesting. I'm a website designer. Pretty much spend all my time on the computer.\n",
      "User: That's cool my mom does the same thing.\n",
      "Bot: That's awesome. I have always had a love for technology.\n",
      "User: Tell me more about yourself.\n",
      "Bot: I really enjoy free diving, how about you, have any hobbies?\n",
      "User: I enjoy hanging with my mother she's my best friend.\n",
      "Bot: That's nice. Moms are pretty cool too.\n",
      "User:\n",
      "Response: Yes, they are. I'm really lucky to have such a close relationship\n",
      "Target Response: Response\n",
      "User: I'm also fascinated with mermaids.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "def prompt_chatgpt_batch(prompt_type, conv_ids, few_shot_no):\n",
    "    dataset = load_dataset(\"bavard/personachat_truecased\", \"full\")\n",
    "    \n",
    "    system_prompts = []\n",
    "    user_prompts = []\n",
    "    target_responses = []\n",
    "\n",
    "    # Collect prompts for all conversation IDs\n",
    "    for conv_id in conv_ids:\n",
    "        system_prompt, user_prompt, target_response = construct_prompt(dataset, conv_id, prompt_type, few_shot_no)\n",
    "        system_prompts.append(system_prompt)\n",
    "        user_prompts.append(user_prompt)\n",
    "        target_responses.append(target_response)\n",
    "    \n",
    "    # Create batch messages for the API request\n",
    "    messages = [{\"role\": \"system\", \"content\": system_prompt} for system_prompt in system_prompts]\n",
    "    messages += [{\"role\": \"user\", \"content\": user_prompt} for user_prompt in user_prompts]\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo-16k\",\n",
    "        messages=messages,\n",
    "        temperature=0.9,\n",
    "        max_tokens=15,  # since persona-chat sets a maximum of 15 words per message\n",
    "    )\n",
    "    \n",
    "    # Process responses and match to the original prompts\n",
    "    responses = [\"\" for _ in conv_ids]\n",
    "    for i, choice in enumerate(response.choices):\n",
    "        index = i % len(conv_ids)\n",
    "        responses[index] = choice.message.content\n",
    "    \n",
    "    # Combine responses with user prompts and target responses\n",
    "    batch_responses = []\n",
    "    for i in range(len(conv_ids)):\n",
    "        batch_responses.append({\n",
    "            \"response\": responses[i],\n",
    "            \"target_response\": target_responses[i],\n",
    "            \"user_prompt\": user_prompts[i]\n",
    "        })\n",
    "    \n",
    "    return batch_responses\n",
    "\n",
    "# Example usage:\n",
    "conv_ids = [1]  # Replace with actual conversation IDs\n",
    "prompt_type = \"context_only\"\n",
    "few_shot_no = 0\n",
    "\n",
    "batch_results = prompt_chatgpt_batch(prompt_type, conv_ids, few_shot_no)\n",
    "for result in batch_results:\n",
    "    print(f\"User Prompt: {result['user_prompt']}\")\n",
    "    print(f\"Response: {result['response']}\")\n",
    "    print(f\"Target Response: {result['target_response']}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dialogue history: \n",
      "Bot: Oh, mojitos are delicious. I\n"
     ]
    }
   ],
   "source": [
    "print(batch_results[0]['response'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|██████████| 131438/131438 [00:02<00:00, 55325.50 examples/s]\n",
      "Filter: 100%|██████████| 7801/7801 [00:00<00:00, 54154.85 examples/s]\n",
      "Filter: 100%|██████████| 131438/131438 [00:02<00:00, 55792.34 examples/s]\n",
      "Filter: 100%|██████████| 7801/7801 [00:00<00:00, 53247.29 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### TASK PROMPT ###\n",
      "Considering the various user profiles and styles depicted in the provided few-shot examples, and the ongoing discussion's context as established in the previous dialogue history, synthesize a coherent and relevant response. This response should be adaptable to the general preferences and communication styles observed in the examples, while seamlessly continuing the dialogue.\n",
      "### USER PROMPT ###\n",
      "Demo 0:\n",
      "Dialogue history: \n",
      "Bot: Hi how are you today?\n",
      "User: Been working all day. Am a mortician. How about you?\n",
      "Bot: Good. That's very interesting. Do you enjoy your work?\n",
      "User: Honestly I do its like helping ppl one last time. Any hobbies?\n",
      "Bot: I really like to draw.\n",
      "User: I have a huge stamp collection. Would love to learn to fly though.\n",
      "Bot: I could never pilot a plane with my sausage fingers.\n",
      "User: Do you have a favorite movie? I could watch dirty Harry everyday.\n",
      "Bot: I love to watch action movies.\n",
      "User: Same! What do you do for work?\n",
      "Bot: I am a comedian. I try to make people laugh.\n",
      "User: How nice I love comedy. Always cheers me up.\n",
      "Bot: What kinds of stamps do you collect?\n",
      "User: Mostly vintage ones. They can be pretty pricey. Do u collect anything?\n",
      "\n",
      "Dialogue history: \n",
      "Bot: Hey! How are you? I'm so sad.\n",
      "User: Sorry to hear that why want to eat pumpkin with me it will make you feel better.\n",
      "Bot: Sure, as long as it isn't mexican food, I hate that stuff.\n",
      "User: What about shrimp do you like that.\n",
      "Bot: Yes! Shrimp and skiing my two favorites.\n",
      "User: Those sound like my idea of fun.\n",
      "Bot: Right! Wish my wife was cool like you! She hates I travel for work.\n",
      "User: I like to travel with my beagle.\n",
      "Bot: I've t go to Mexico every other month, tomorrow is my fifth trip in 3 months.\n",
      "User: I can not stand Adobe.\n",
      "Bot: I know right! That is why my wife hates me too.\n",
      "User: I am sorry to hear that.\n",
      "Bot: Do you like music? I love it!\n",
      "User: I love music of all types.\n",
      "Bot: Me too! I like to listen to it while I ski.\n",
      "User:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from datasets import load_dataset\n",
    "\n",
    "def get_random_conv_id(pool, num_conv):\n",
    "    random_values = random.sample(pool, num_conv)\n",
    "\n",
    "    for value in random_values:\n",
    "        pool.remove(value)\n",
    "    \n",
    "    return random_values\n",
    "\n",
    "def extract_format_data(dataset, conv_id):\n",
    "    dialog = dataset.filter(lambda example:example['conv_id']== conv_id)\n",
    "\n",
    "    persona = dialog['train']['personality'][-1]\n",
    "    history_convo = dialog['train']['history'][-1]\n",
    "    usr_response = dialog['train']['candidates'][-1][-1]\n",
    "\n",
    "    #preprocess the persona\n",
    "    persona_processed = \"User Persona: \\n\"\n",
    "    for sen in persona:\n",
    "        persona_processed += sen + \"\\n\"\n",
    "\n",
    "    #preprocess the history conversation\n",
    "    history_convo_processed = \"Dialogue history: \\n\"\n",
    "    \n",
    "    #concat all history except the last utter from the bot\n",
    "    for i in range(0,len(history_convo)-1,2):\n",
    "        bot_uttr = \"Bot: \" + history_convo[i]\n",
    "        user_uttr = \"User: \" + history_convo[i+1]\n",
    "        full_uttr = bot_uttr + \"\\n\" + user_uttr + \"\\n\"\n",
    "        history_convo_processed += full_uttr\n",
    "\n",
    "    #add the last utter from bot\n",
    "    history_convo_processed += \"Bot: \" + history_convo[-1] + \"\\n\"\n",
    "\n",
    "    #preprocess the user response\n",
    "    usr_response_processed = \"User: \" + usr_response + \"\\n\"\n",
    "    return [persona_processed, history_convo_processed, usr_response_processed]\n",
    "\n",
    "# materials[0] is persona, materials[1] is history conversation, materials[2] is user response\n",
    "def create_example(dataset, conv_id, implicit=False):\n",
    "    materials = extract_format_data(dataset, conv_id)\n",
    "    if implicit:\n",
    "        return materials[1] + materials[2]\n",
    "    else:\n",
    "        return materials[0] + materials[1] + materials[2]\n",
    "    \n",
    "\n",
    "def create_few_shot_examples(dataset, conv_id, few_shot_no, implicit=False):\n",
    "    max_conv_id = dataset['train']['conv_id'][-1]\n",
    "    pool = [num for num in range(1, max_conv_id + 1) if num != conv_id]\n",
    "    random_conv_ids = get_random_conv_id(pool, few_shot_no)\n",
    "\n",
    "    few_shot_examples = \"\"\n",
    "    for index, conv_id in enumerate(random_conv_ids):\n",
    "        few_shot_examples += f\"Demo {index}:\\n\" + create_example(dataset, conv_id, implicit) + \"\\n\"\n",
    "    return few_shot_examples\n",
    "\n",
    "\n",
    "def construct_prompt(dataset, conv_id, prompt_type, few_shot_no=1, print_output= False):\n",
    "    max_conv_id = dataset['train']['conv_id'][-1]\n",
    "    materials = extract_format_data(dataset, conv_id)\n",
    "    system_prompt = \"\"\n",
    "    user_prompt = \"\"\n",
    "    few_shot_examples = \"\"\n",
    "\n",
    "    # only history dialogue\n",
    "    if prompt_type == \"context_only\":\n",
    "        user_prompt += materials[1] + \"User:\"\n",
    "\n",
    "    # task prompt, history dialogue\n",
    "    if prompt_type == \"task_prompt_context_implicit\":\n",
    "        #version 1 Based on the previous conversation history, generate a response for the user that aligns with their profile and the current context of the discussion.\n",
    "        system_prompt += \"Considering the user's profile and the ongoing discussion's context as established in the previous dialogue history, craft a response that is coherent, relevant, and tailored to the user's interests and style of communication.\"\n",
    "        user_prompt += materials[1] + \"User:\"\n",
    "\n",
    "    # task prompt, persona, and history dialogue\n",
    "    if prompt_type == \"task_prompt_context_explicit\":\n",
    "        system_prompt += \"Given the user's profile as outlined in the provided persona information, and considering the context of the ongoing discussion from the previous dialogue history, craft a response that is specifically tailored to resonate with the user's explicit characteristics and maintains the continuity of the dialogue.\"\n",
    "        user_prompt += materials[0]+ materials[1] + \"User:\"\n",
    "\n",
    "    # few-shot demos, history dialogue\n",
    "    if prompt_type == \"few_shot_implicit\":\n",
    "        system_prompt += \"Considering the various user profiles and styles depicted in the provided few-shot examples, and the ongoing discussion's context as established in the previous dialogue history, synthesize a coherent and relevant response. This response should be adaptable to the general preferences and communication styles observed in the examples, while seamlessly continuing the dialogue.\"\n",
    "\n",
    "        #set the implicit flag to True to exclude persona in few-shot demos\n",
    "        few_shot_examples = create_few_shot_examples(dataset, conv_id, few_shot_no, implicit=False)\n",
    "        user_prompt += few_shot_examples + materials[1]+ \"User:\"\n",
    "\n",
    "    if print_output:\n",
    "        print(\"### TASK PROMPT ###\\n\" + system_prompt)\n",
    "        print(\"### USER PROMPT ###\\n\" + user_prompt)\n",
    "    \n",
    "    return system_prompt, user_prompt, materials[2]\n",
    "\n",
    "\n",
    "#context_only, task_prompt_context_implicit, task_prompt_context_explicit, few_shot_implicit\n",
    "def test():\n",
    "    dataset = load_dataset(\"bavard/personachat_truecased\", \"full\")\n",
    "    result = construct_prompt(dataset, 174, \"few_shot_implicit\",print_output=True)\n",
    "    return result\n",
    "results = test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Dialogue history: \\nBot: Hi, how are you doing today?\\nUser: I am spending time with my 4 sisters what are you up to.\\nBot: Wow, four sisters. Just watching game of thrones.\\nUser: That is a good show I watch that while drinking iced tea.\\nBot: I agree. What do you do for a living?\\nUser: I'm a researcher I'm researching the fact that mermaids are real.\\nBot: Interesting. I'm a website designer. Pretty much spend all my time on the computer.\\nUser: That's cool my mom does the same thing.\\nBot: That's awesome. I have always had a love for technology.\\nUser: Tell me more about yourself.\\nBot: I really enjoy free diving, how about you, have any hobbies?\\nUser: I enjoy hanging with my mother she's my best friend.\\nBot: That's nice. Moms are pretty cool too.\\nUser:\""
      ]
     },
     "execution_count": 301,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
