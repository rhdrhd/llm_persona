{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. import dataset and load sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['personality', 'candidates', 'history', 'conv_id', 'utterance_idx'],\n",
      "        num_rows: 14\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['personality', 'candidates', 'history', 'conv_id', 'utterance_idx'],\n",
      "        num_rows: 15\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"bavard/personachat_truecased\", \"sample\")\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'personality': ['I like to remodel homes.', 'I like to go hunting.', 'I like to shoot a bow.', 'My favorite holiday is halloween.'], 'candidates': ['My mom was single with 3 boys, so we never left the projects.', 'I try to wear all black every day. It makes me feel comfortable.', 'Well nursing stresses you out so I wish luck with sister.', 'Yeah just want to pick up Nba nfl getting old.', 'I really like Celine Dion. What about you?', 'No. I live near farms.', \"I wish I had a daughter, I'm a boy mom. They're beautiful boys though still lucky.\", 'Yeah when I get bored I play gone with the wind my favorite movie.', \"Hi how are you? I'm eating dinner with my hubby and 2 kids.\", 'Were you married to your high school sweetheart? I was.', 'That is great to hear! Are you a competitive rider?', \"Hi, I'm doing ok. I'm a banker. How about you?\", \"I'm 5 years old.\", 'Hi there. How are you today?', 'I totally understand how stressful that can be.', 'Yeah sometimes you do not know what you are actually watching.', 'Mother taught me to cook! We are looking for an exterminator.', 'I enjoy romantic movie. What is your favorite season? Mine is summer.', 'Editing photos takes a lot of work.', 'You must be very fast. Hunting is one of my favorite hobbies.'], 'history': [\"Hi, how are you doing? I'm getting ready to do some cheetah chasing to stay in shape.\"], 'conv_id': 0, 'utterance_idx': 0}\n"
     ]
    }
   ],
   "source": [
    "print(dataset[\"train\"][0])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Define history conversation, response, and persona"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "#select conversation with certain id\n",
    "conv_id = 6\n",
    "dataset = load_dataset(\"bavard/personachat_truecased\", \"full\")\n",
    "dialog = dataset.filter(lambda example:example['conv_id']== conv_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User Persona:  ['I have a boxer dog.', 'I like baths.', 'I like to listen to music.', 'My father lives in China.'] \n",
      "\n",
      "History Conversation:  [\"Rock on, I'm listening to my favorite band guns and roses.\", 'No kidding? I was just listening to the same thing while taking a bath.', 'Of course. I love to listen to rock.', 'Man my boxer just peed on the carpet!', \"Well I'm into black everything. So at least it wouldn't show on my black carpet.\", 'Ll. I love black too! Guess I was playing my music too loud.', \"I've a black car, purse, wear all black.\", 'Maybe I can borrow something as I am packing to visit my dad in China.', 'Wow, does he live there or work?', 'Live. Moved there about ten years ago for a computer tech job.', 'Have you visited him there before?', 'Once. You cannot even throw a gum wrapper or you can get arrested.', 'Sounds a bit scary. I ve never been.'] \n",
      "\n",
      "Response:  Well not too much crime there, but a lot of people.\n"
     ]
    }
   ],
   "source": [
    "#select persona \n",
    "persona = dialog['train']['personality'][-1]\n",
    "print(\"User Persona: \", persona, \"\\n\")\n",
    "\n",
    "#select the history conversation\n",
    "history_convo = dialog['train']['history'][-1]\n",
    "print(\"History Conversation: \", history_convo, \"\\n\")\n",
    "\n",
    "#select the response\n",
    "usr_response = dialog['train']['candidates'][-1][-1]\n",
    "print(\"Response: \",usr_response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Preprocess the text to make it suitable for prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dialogue history: \n",
      "Bot: Rock on, I'm listening to my favorite band guns and roses.\n",
      "User: No kidding? I was just listening to the same thing while taking a bath.\n",
      "Bot: Of course. I love to listen to rock.\n",
      "User: Man my boxer just peed on the carpet!\n",
      "Bot: Well I'm into black everything. So at least it wouldn't show on my black carpet.\n",
      "User: Ll. I love black too! Guess I was playing my music too loud.\n",
      "Bot: I've a black car, purse, wear all black.\n",
      "User: Maybe I can borrow something as I am packing to visit my dad in China.\n",
      "Bot: Wow, does he live there or work?\n",
      "User: Live. Moved there about ten years ago for a computer tech job.\n",
      "Bot: Have you visited him there before?\n",
      "User: Once. You cannot even throw a gum wrapper or you can get arrested.\n",
      "Bot: Sounds a bit scary. I ve never been.\n",
      "\n",
      "Response\n",
      "User:\n"
     ]
    }
   ],
   "source": [
    "#preprocess the persona\n",
    "persona_processed = \"User Persona: \\n\"\n",
    "for sen in persona:\n",
    "    persona_processed += sen + \"\\n\"\n",
    "\n",
    "#preprocess the history conversation\n",
    "history_convo_processed = \"Dialogue history: \\n\"\n",
    "\n",
    "#concat all history except the last utter from the bot\n",
    "for i in range(0,len(history_convo)-1,2):\n",
    "    bot_uttr = \"Bot: \" + history_convo[i]\n",
    "    user_uttr = \"User: \" + history_convo[i+1]\n",
    "    full_uttr = bot_uttr + \"\\n\" + user_uttr + \"\\n\"\n",
    "    history_convo_processed += full_uttr\n",
    "\n",
    "#add the last utter from bot\n",
    "history_convo_processed += \"Bot: \" + history_convo[-1] + \"\\n\"\n",
    "\n",
    "#concat prompt\n",
    "prompt = persona_processed + \"\\n\" + history_convo_processed + \"\\nResponse\\nUser:\"\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "\n",
    "os.environ['OPENAI_API_KEY'] = 'sk-proj-zgpS1dH01GeQiQzhICjNT3BlbkFJq8oC1i8atPEckRUxahYK'\n",
    "client = OpenAI()\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo\",  # Specify the model\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"According to the history conversation provided, pretend to speak like a human user and generate a response.\"},\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ],\n",
    "    temperature=0,  # Adjust based on how creative you want the AI to be\n",
    "    max_tokens=150, # Adjust based on how long you expect responses to be\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"id\": \"chatcmpl-9ikTtLOpcukHYtnHPaUuqlCtz9enM\",\n",
      "    \"choices\": [\n",
      "        {\n",
      "            \"finish_reason\": \"stop\",\n",
      "            \"index\": 0,\n",
      "            \"logprobs\": null,\n",
      "            \"message\": {\n",
      "                \"content\": \"China is definitely a unique experience with its strict rules. It's important to be mindful of the cultural differences when visiting.\",\n",
      "                \"role\": \"assistant\",\n",
      "                \"function_call\": null,\n",
      "                \"tool_calls\": null\n",
      "            }\n",
      "        }\n",
      "    ],\n",
      "    \"created\": 1720452133,\n",
      "    \"model\": \"gpt-3.5-turbo-0125\",\n",
      "    \"object\": \"chat.completion\",\n",
      "    \"service_tier\": null,\n",
      "    \"system_fingerprint\": null,\n",
      "    \"usage\": {\n",
      "        \"completion_tokens\": 24,\n",
      "        \"prompt_tokens\": 233,\n",
      "        \"total_tokens\": 257\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "print(json.dumps(json.loads(response.model_dump_json()), indent=4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "China is definitely a unique experience with its strict rules. It's important to be mindful of the cultural differences when visiting.\n",
      "Well not too much crime there, but a lot of people.\n"
     ]
    }
   ],
   "source": [
    "response_chatgpt = response.choices[0].message.content\n",
    "\n",
    "print(response_chatgpt)\n",
    "\n",
    "print(usr_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import random\n",
    "\n",
    "def get_random_conv_id(pool, num_conv):\n",
    "    random_values = random.sample(pool, num_conv)\n",
    "\n",
    "    for value in random_values:\n",
    "        pool.remove(value)\n",
    "    \n",
    "    return random_values\n",
    "\n",
    "def extract_format_data(dataset, conv_id):\n",
    "    dialog = dataset.filter(lambda example:example['conv_id']== conv_id)\n",
    "\n",
    "    persona = dialog['train']['personality'][-1]\n",
    "    history_convo = dialog['train']['history'][-1]\n",
    "    usr_response = dialog['train']['candidates'][-1][-1]\n",
    "\n",
    "    #preprocess the persona\n",
    "    persona_processed = \"User Persona: \\n\"\n",
    "    for sen in persona:\n",
    "        persona_processed += sen + \"\\n\"\n",
    "\n",
    "    #preprocess the history conversation\n",
    "    history_convo_processed = \"Dialogue history: \\n\"\n",
    "\n",
    "    #concat all history except the last utter from the bot\n",
    "    for i in range(0,len(history_convo)-1,2):\n",
    "        bot_uttr = \"Bot: \" + history_convo[i]\n",
    "        user_uttr = \"User: \" + history_convo[i+1]\n",
    "        full_uttr = bot_uttr + \"\\n\" + user_uttr + \"\\n\"\n",
    "        history_convo_processed += full_uttr\n",
    "\n",
    "    #add the last utter from bot\n",
    "    history_convo_processed += \"Bot: \" + history_convo[-1] + \"\\n\"\n",
    "\n",
    "    #preprocess the user response\n",
    "    usr_response_processed = \"Response\\nUser: \" + usr_response + \"\\n\"\n",
    "    return [persona_processed, history_convo_processed, usr_response_processed]\n",
    "\n",
    "def create_example(dataset, conv_id, implicit=False):\n",
    "    materials = extract_format_data(dataset, conv_id)\n",
    "    if implicit:\n",
    "        return materials[1] + materials[2]\n",
    "    else:\n",
    "        return materials[0] + materials[1] + materials[2]\n",
    "    return example\n",
    "\n",
    "def create_few_shot_examples(dataset, conv_id, few_shot_no, implicit=False):\n",
    "    max_conv_id = dataset['train']['conv_id'][-1]\n",
    "    pool = [num for num in range(1, max_conv_id + 1) if num != conv_id]\n",
    "    random_conv_ids = get_random_conv_id(pool, few_shot_no)\n",
    "\n",
    "    few_shot_examples = \"\"\n",
    "    for index, conv_id in enumerate(random_conv_ids):\n",
    "        few_shot_examples += f\"Demo {index}:\\n\" + create_example(dataset, conv_id, implicit) + \"\\n\"\n",
    "    return few_shot_examples\n",
    "\n",
    "\n",
    "def construct_prompt(dataset, conv_id, prompt_type, few_shot_no):\n",
    "    max_conv_id = dataset['train']['conv_id'][-1]\n",
    "    materials = extract_format_data(dataset, conv_id)\n",
    "    system_prompt = \"\"\n",
    "    user_prompt = \"\"\n",
    "    few_shot_examples = \"\"\n",
    "\n",
    "    if prompt_type == \"context_only\":\n",
    "        user_prompt += materials[1] + \"User:\"\n",
    "\n",
    "    if prompt_type == \"task_prompt_context_implicit\":\n",
    "        system_prompt += \"According to the history conversation provided, pretend to speak like a human user and generate a response.\"\n",
    "        user_prompt += materials[1] + \"User:\"\n",
    "\n",
    "    if prompt_type == \"task_prompt_context_explicit\":\n",
    "        system_prompt += \"According to the persona and history conversation provided, pretend to speak like a human user and generate a response.\"\n",
    "        user_prompt += materials[0]+ materials[1] + \"User:\"\n",
    "\n",
    "    if prompt_type == \"few_shot_implicit\":\n",
    "        system_prompt += \"According to the few shot demos provided, pretend to speak like User and generate a response matching the context.\"\n",
    "\n",
    "        few_shot_examples = create_few_shot_examples(dataset, conv_id, few_shot_no,implicit=True)\n",
    "        user_prompt += few_shot_examples + materials[1]+ \"User:\"\n",
    "\n",
    "    return system_prompt, user_prompt, materials[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|██████████| 131438/131438 [00:02<00:00, 52279.80 examples/s]\n",
      "Filter: 100%|██████████| 7801/7801 [00:00<00:00, 52893.94 examples/s]\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(\"bavard/personachat_truecased\", \"full\")\n",
    "system_p, user_p, target_response = construct_prompt(dataset, 6, \"few_shot_implicit\", 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'According to the few shot demos provided, pretend to speak like User and generate a response matching the context.'"
      ]
     },
     "execution_count": 278,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "system_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "\n",
    "os.environ['OPENAI_API_KEY'] = 'sk-proj-zgpS1dH01GeQiQzhICjNT3BlbkFJq8oC1i8atPEckRUxahYK'\n",
    "client = OpenAI()\n",
    "\n",
    "def prompt_chatgpt(prompt_type, conv_id, few_shot_no=0):\n",
    "    dataset = load_dataset(\"bavard/personachat_truecased\", \"full\")\n",
    "    system_prompt, user_prompt, target_response= construct_prompt(dataset, conv_id, prompt_type, few_shot_no)\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo-16k\",  # Specify the model\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ],\n",
    "        temperature=0.9,  # Adjust based on how creative you want the AI to be\n",
    "        max_tokens=15, # Adjust based on how long you expect responses to be\n",
    "    )\n",
    "    return response.choices[0].message.content, user_prompt, target_response\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "response, user_prompt, target_repsonse= prompt_chatgpt(\"task_prompt_context_explicit\", 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User Persona: \n",
      "I have a boxer dog.\n",
      "I like baths.\n",
      "I like to listen to music.\n",
      "My father lives in China.\n",
      "Dialogue history: \n",
      "Bot: Rock on, I'm listening to my favorite band guns and roses.\n",
      "User: No kidding? I was just listening to the same thing while taking a bath.\n",
      "Bot: Of course. I love to listen to rock.\n",
      "User: Man my boxer just peed on the carpet!\n",
      "Bot: Well I'm into black everything. So at least it wouldn't show on my black carpet.\n",
      "User: Ll. I love black too! Guess I was playing my music too loud.\n",
      "Bot: I've a black car, purse, wear all black.\n",
      "User: Maybe I can borrow something as I am packing to visit my dad in China.\n",
      "Bot: Wow, does he live there or work?\n",
      "User: Live. Moved there about ten years ago for a computer tech job.\n",
      "Bot: Have you visited him there before?\n",
      "User: Once. You cannot even throw a gum wrapper or you can get arrested.\n",
      "Bot: Sounds a bit scary. I ve never been.\n",
      "User:\n",
      "It can be a bit intimidating at times, but overall it's a fascinating\n",
      "Response\n",
      "User: Well not too much crime there, but a lot of people.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(user_prompt)\n",
    "print(response)\n",
    "print(target_repsonse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"It can be a bit intimidating at times, but overall it's a fascinating\""
      ]
     },
     "execution_count": 286,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "131438\n"
     ]
    }
   ],
   "source": [
    "max_conv_id = len(dataset['train']['conv_id'])\n",
    "print(max_conv_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User Persona: \n",
      "I have a boxer dog.\n",
      "I like baths.\n",
      "I like to listen to music.\n",
      "My father lives in China.\n",
      "Dialogue history: \n",
      "Bot: Rock on, I'm listening to my favorite band guns and roses.\n",
      "User: No kidding? I was just listening to the same thing while taking a bath.\n",
      "Bot: Of course. I love to listen to rock.\n",
      "User: Man my boxer just peed on the carpet!\n",
      "Bot: Well I'm into black everything. So at least it wouldn't show on my black carpet.\n",
      "User: Ll. I love black too! Guess I was playing my music too loud.\n",
      "Bot: I've a black car, purse, wear all black.\n",
      "User: Maybe I can borrow something as I am packing to visit my dad in China.\n",
      "Bot: Wow, does he live there or work?\n",
      "User: Live. Moved there about ten years ago for a computer tech job.\n",
      "Bot: Have you visited him there before?\n",
      "User: Once. You cannot even throw a gum wrapper or you can get arrested.\n",
      "Bot: Sounds a bit scary. I ve never been.\n",
      "Response:\n",
      "User: Well not too much crime there, but a lot of people.\n"
     ]
    }
   ],
   "source": [
    "print(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rouge_score import rouge_scorer\n",
    "\n",
    "scorer = rouge_scorer.RougeScorer(['rouge1', 'rougeL'], use_stemmer=True)\n",
    "scores = scorer.score('The quick brown fox jumps over the lazy dog',\n",
    "                      'The quick brown dog jumps on the log.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7058823529411765"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores['rouge1'].fmeasure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/zarius/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt/PY3/english.pickle\u001b[0m\n\n  Searched in:\n    - '/Users/zarius/nltk_data'\n    - '/Users/zarius/miniconda3/envs/llm_persona/nltk_data'\n    - '/Users/zarius/miniconda3/envs/llm_persona/share/nltk_data'\n    - '/Users/zarius/miniconda3/envs/llm_persona/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - ''\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[252], line 29\u001b[0m\n\u001b[1;32m     26\u001b[0m persona \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mI love playing basketball and enjoy watching movies.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     27\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBasketball is my favorite sport. I also love watching movies in my free time.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 29\u001b[0m p_cover_score \u001b[38;5;241m=\u001b[39m \u001b[43mp_cover\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpersona\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mP-Cover Score: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mp_cover_score\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[252], line 10\u001b[0m, in \u001b[0;36mp_cover\u001b[0;34m(persona, response)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mp_cover\u001b[39m(persona, response):\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;66;03m# Tokenize and remove stop words\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m     persona_tokens \u001b[38;5;241m=\u001b[39m [word \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m \u001b[43mnltk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mword_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpersona\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlower\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m word\u001b[38;5;241m.\u001b[39misalnum() \u001b[38;5;129;01mand\u001b[39;00m word \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m stop_words]\n\u001b[1;32m     11\u001b[0m     response_tokens \u001b[38;5;241m=\u001b[39m [word \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m nltk\u001b[38;5;241m.\u001b[39mword_tokenize(response\u001b[38;5;241m.\u001b[39mlower()) \u001b[38;5;28;01mif\u001b[39;00m word\u001b[38;5;241m.\u001b[39misalnum() \u001b[38;5;129;01mand\u001b[39;00m word \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m stop_words]\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;66;03m# Calculate TF-IDF for the persona terms\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/llm_persona/lib/python3.9/site-packages/nltk/tokenize/__init__.py:129\u001b[0m, in \u001b[0;36mword_tokenize\u001b[0;34m(text, language, preserve_line)\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mword_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m, preserve_line\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m    115\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;124;03m    Return a tokenized copy of *text*,\u001b[39;00m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;124;03m    :type preserve_line: bool\u001b[39;00m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 129\u001b[0m     sentences \u001b[38;5;241m=\u001b[39m [text] \u001b[38;5;28;01mif\u001b[39;00m preserve_line \u001b[38;5;28;01melse\u001b[39;00m \u001b[43msent_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[1;32m    131\u001b[0m         token \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m sentences \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m _treebank_word_tokenizer\u001b[38;5;241m.\u001b[39mtokenize(sent)\n\u001b[1;32m    132\u001b[0m     ]\n",
      "File \u001b[0;32m~/miniconda3/envs/llm_persona/lib/python3.9/site-packages/nltk/tokenize/__init__.py:106\u001b[0m, in \u001b[0;36msent_tokenize\u001b[0;34m(text, language)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msent_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     97\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;124;03m    Return a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;124;03m    :param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 106\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtokenizers/punkt/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mlanguage\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.pickle\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    107\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer\u001b[38;5;241m.\u001b[39mtokenize(text)\n",
      "File \u001b[0;32m~/miniconda3/envs/llm_persona/lib/python3.9/site-packages/nltk/data.py:750\u001b[0m, in \u001b[0;36mload\u001b[0;34m(resource_url, format, cache, verbose, logic_parser, fstruct_reader, encoding)\u001b[0m\n\u001b[1;32m    747\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<<Loading \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresource_url\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m>>\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    749\u001b[0m \u001b[38;5;66;03m# Load the resource.\u001b[39;00m\n\u001b[0;32m--> 750\u001b[0m opened_resource \u001b[38;5;241m=\u001b[39m \u001b[43m_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresource_url\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    752\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mformat\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    753\u001b[0m     resource_val \u001b[38;5;241m=\u001b[39m opened_resource\u001b[38;5;241m.\u001b[39mread()\n",
      "File \u001b[0;32m~/miniconda3/envs/llm_persona/lib/python3.9/site-packages/nltk/data.py:876\u001b[0m, in \u001b[0;36m_open\u001b[0;34m(resource_url)\u001b[0m\n\u001b[1;32m    873\u001b[0m protocol, path_ \u001b[38;5;241m=\u001b[39m split_resource_url(resource_url)\n\u001b[1;32m    875\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m protocol \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m protocol\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnltk\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 876\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mopen()\n\u001b[1;32m    877\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m protocol\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfile\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    878\u001b[0m     \u001b[38;5;66;03m# urllib might not use mode='rb', so handle this one ourselves:\u001b[39;00m\n\u001b[1;32m    879\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m find(path_, [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m])\u001b[38;5;241m.\u001b[39mopen()\n",
      "File \u001b[0;32m~/miniconda3/envs/llm_persona/lib/python3.9/site-packages/nltk/data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    581\u001b[0m sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m70\u001b[39m\n\u001b[1;32m    582\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 583\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt/PY3/english.pickle\u001b[0m\n\n  Searched in:\n    - '/Users/zarius/nltk_data'\n    - '/Users/zarius/miniconda3/envs/llm_persona/nltk_data'\n    - '/Users/zarius/miniconda3/envs/llm_persona/share/nltk_data'\n    - '/Users/zarius/miniconda3/envs/llm_persona/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - ''\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def p_cover(persona, response):\n",
    "    # Tokenize and remove stop words\n",
    "    persona_tokens = [word for word in nltk.word_tokenize(persona.lower()) if word.isalnum() and word not in stop_words]\n",
    "    response_tokens = [word for word in nltk.word_tokenize(response.lower()) if word.isalnum() and word not in stop_words]\n",
    "    \n",
    "    # Calculate TF-IDF for the persona terms\n",
    "    vectorizer = TfidfVectorizer(vocabulary=set(persona_tokens))\n",
    "    tfidf_matrix = vectorizer.fit_transform([response])\n",
    "    \n",
    "    # Extract the TF-IDF scores for the persona terms in the response\n",
    "    tfidf_scores = tfidf_matrix.toarray()[0]\n",
    "    \n",
    "    # Calculate the P-Cover score\n",
    "    p_cover_score = sum(tfidf_scores) / len(persona_tokens)\n",
    "    \n",
    "    return p_cover_score\n",
    "\n",
    "# Example usage\n",
    "persona = \"I love playing basketball and enjoy watching movies.\"\n",
    "response = \"Basketball is my favorite sport. I also love watching movies in my free time.\"\n",
    "\n",
    "p_cover_score = p_cover(persona, response)\n",
    "print(f\"P-Cover Score: {p_cover_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from rouge_score import rouge_scorer\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "\n",
    "\n",
    "# implementation of P-Cover\n",
    "def calculate_p_cover(persona, response):\n",
    "    return 0\n",
    "\n",
    "# implementation of BLEU-1, BLEU-2, BLEU-3, and BLEU-4\n",
    "def calculate_bleu(reference_sentence, candidate_sentence):\n",
    "    reference = [reference_sentence.split()]\n",
    "    candidate = candidate_sentence.split()\n",
    "    \n",
    "    # Weights for BLEU-1, BLEU-2, BLEU-3, and BLEU-4\n",
    "    weights = [\n",
    "        (1.0, 0, 0, 0),        \n",
    "        (0.5, 0.5, 0, 0),        \n",
    "        (1.0 / 3, 1.0 / 3, 1.0 / 3, 0), \n",
    "        (0.25, 0.25, 0.25, 0.25) \n",
    "    ]\n",
    "    \n",
    "    # Use smoothing to handle cases with few n-grams\n",
    "    smoothing_function = SmoothingFunction()\n",
    "    scores = sentence_bleu(reference, candidate, weights, smoothing_function=smoothing_function.method1)\n",
    "    return scores\n",
    "\n",
    "# rouge1, rouge2, rougeL\n",
    "def calculate_rouge(reference_sentence, candidate_sentence):\n",
    "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "    scores = scorer.score(reference_sentence, candidate_sentence)\n",
    "    return scores\n",
    "\n",
    "def cosine_similarity_embeddings(sentence1, sentence2, model_name='all-MiniLM-L6-v2'):\n",
    "    model = SentenceTransformer(model_name)\n",
    "    embeddings = model.encode([sentence1, sentence2])\n",
    "    cos_sim = cosine_similarity([embeddings[0]], [embeddings[1]])[0][0]\n",
    "    return cos_sim\n",
    "\n",
    "def distinct_1(sentence):\n",
    "    words = sentence.split()\n",
    "    unique_unigrams = set(words)\n",
    "    total_unigrams = len(words)\n",
    "    if total_unigrams == 0:\n",
    "        return 0.0\n",
    "    distinct_1_score = len(unique_unigrams) / total_unigrams\n",
    "    return distinct_1_score\n",
    "\n",
    "def distinct_2(sentence):\n",
    "    words = sentence.split()\n",
    "    bigrams = list(nltk.bigrams(words))\n",
    "    unique_bigrams = set(bigrams)\n",
    "    total_bigrams = len(bigrams)\n",
    "    if total_bigrams == 0:\n",
    "        return 0.0\n",
    "    distinct_2_score = len(unique_bigrams) / total_bigrams\n",
    "    return distinct_2_score\n",
    "\n",
    "def calculate_metrics(reference_sentence, candidate_sentence):\n",
    "    bleu_score = calculate_bleu(reference_sentence, candidate_sentence)\n",
    "    rouge_scores = calculate_rouge(reference_sentence, candidate_sentence)\n",
    "    cosine_similarity = cosine_similarity_embeddings(reference_sentence, candidate_sentence)\n",
    "    distinct_1_score = distinct_1(candidate_sentence)\n",
    "    distinct_2_score = distinct_2(candidate_sentence)\n",
    "\n",
    "    return {\n",
    "        'BLEU-1': bleu_score[0],\n",
    "        'BLEU-2': bleu_score[1],\n",
    "        'BLEU-3': bleu_score[2],\n",
    "        'BLEU-4': bleu_score[3],\n",
    "        'ROUGE-1': rouge_scores['rouge1'].fmeasure,\n",
    "        'ROUGE-2': rouge_scores['rouge2'].fmeasure,\n",
    "        'ROUGE-L': rouge_scores['rougeL'].fmeasure,\n",
    "        'Cosine Similarity': cosine_similarity,\n",
    "        'Distinct-1': distinct_1_score,\n",
    "        'Distinct-2': distinct_2_score\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'BLEU-1': 0.8187307530779819,\n",
       " 'BLEU-2': 0.7090416310250969,\n",
       " 'BLEU-3': 0.6498270293573523,\n",
       " 'BLEU-4': 0.5789300674674098,\n",
       " 'ROUGE-1': 0.9090909090909091,\n",
       " 'ROUGE-2': 0.6666666666666665,\n",
       " 'ROUGE-L': 0.9090909090909091,\n",
       " 'Cosine Similarity': 0.9925266,\n",
       " 'Distinct-1': 1.0,\n",
       " 'Distinct-2': 1.0}"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reference_sentence = \"the cat is on the mat\"\n",
    "candidate_sentence = \"the cat is on mat\"\n",
    "calculate_metrics(reference_sentence, candidate_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_stories = 10\n",
    "prompts = [\"Once upon a time,\"] * num_stories\n",
    "stories = [\"\"] * len(prompts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['', '', '', '', '', '', '', '', '', '']"
      ]
     },
     "execution_count": 262,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User Prompt: Dialogue history: \n",
      "Bot: Hi, how are you doing today?\n",
      "User: I am spending time with my 4 sisters what are you up to.\n",
      "Bot: Wow, four sisters. Just watching game of thrones.\n",
      "User: That is a good show I watch that while drinking iced tea.\n",
      "Bot: I agree. What do you do for a living?\n",
      "User: I'm a researcher I'm researching the fact that mermaids are real.\n",
      "Bot: Interesting. I'm a website designer. Pretty much spend all my time on the computer.\n",
      "User: That's cool my mom does the same thing.\n",
      "Bot: That's awesome. I have always had a love for technology.\n",
      "User: Tell me more about yourself.\n",
      "Bot: I really enjoy free diving, how about you, have any hobbies?\n",
      "User: I enjoy hanging with my mother she's my best friend.\n",
      "Bot: That's nice. Moms are pretty cool too.\n",
      "User:\n",
      "Response: Yes, they are. I'm really lucky to have such a close relationship\n",
      "Target Response: Response\n",
      "User: I'm also fascinated with mermaids.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "def prompt_chatgpt_batch(prompt_type, conv_ids, few_shot_no):\n",
    "    dataset = load_dataset(\"bavard/personachat_truecased\", \"full\")\n",
    "    \n",
    "    system_prompts = []\n",
    "    user_prompts = []\n",
    "    target_responses = []\n",
    "\n",
    "    # Collect prompts for all conversation IDs\n",
    "    for conv_id in conv_ids:\n",
    "        system_prompt, user_prompt, target_response = construct_prompt(dataset, conv_id, prompt_type, few_shot_no)\n",
    "        system_prompts.append(system_prompt)\n",
    "        user_prompts.append(user_prompt)\n",
    "        target_responses.append(target_response)\n",
    "    \n",
    "    # Create batch messages for the API request\n",
    "    messages = [{\"role\": \"system\", \"content\": system_prompt} for system_prompt in system_prompts]\n",
    "    messages += [{\"role\": \"user\", \"content\": user_prompt} for user_prompt in user_prompts]\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo-16k\",\n",
    "        messages=messages,\n",
    "        temperature=0.9,\n",
    "        max_tokens=15,  # since persona-chat sets a maximum of 15 words per message\n",
    "    )\n",
    "    \n",
    "    # Process responses and match to the original prompts\n",
    "    responses = [\"\" for _ in conv_ids]\n",
    "    for i, choice in enumerate(response.choices):\n",
    "        index = i % len(conv_ids)\n",
    "        responses[index] = choice.message.content\n",
    "    \n",
    "    # Combine responses with user prompts and target responses\n",
    "    batch_responses = []\n",
    "    for i in range(len(conv_ids)):\n",
    "        batch_responses.append({\n",
    "            \"response\": responses[i],\n",
    "            \"target_response\": target_responses[i],\n",
    "            \"user_prompt\": user_prompts[i]\n",
    "        })\n",
    "    \n",
    "    return batch_responses\n",
    "\n",
    "# Example usage:\n",
    "conv_ids = [1]  # Replace with actual conversation IDs\n",
    "prompt_type = \"context_only\"\n",
    "few_shot_no = 0\n",
    "\n",
    "batch_results = prompt_chatgpt_batch(prompt_type, conv_ids, few_shot_no)\n",
    "for result in batch_results:\n",
    "    print(f\"User Prompt: {result['user_prompt']}\")\n",
    "    print(f\"Response: {result['response']}\")\n",
    "    print(f\"Target Response: {result['target_response']}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dialogue history: \n",
      "Bot: Oh, mojitos are delicious. I\n"
     ]
    }
   ],
   "source": [
    "print(batch_results[0]['response'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zarius/miniconda3/envs/llm_persona/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from datasets import load_dataset\n",
    "\n",
    "def get_random_conv_id(pool, num_conv):\n",
    "    random_values = random.sample(pool, num_conv)\n",
    "\n",
    "    for value in random_values:\n",
    "        pool.remove(value)\n",
    "    \n",
    "    return random_values\n",
    "\n",
    "def extract_format_data(dataset, conv_id):\n",
    "    dialog = dataset.filter(lambda example:example['conv_id']== conv_id)\n",
    "\n",
    "    persona = dialog['train']['personality'][-1]\n",
    "    history_convo = dialog['train']['history'][-1]\n",
    "    usr_response = dialog['train']['candidates'][-1][-1]\n",
    "\n",
    "    unprocessed = [persona, history_convo, usr_response]\n",
    "\n",
    "    #preprocess the persona\n",
    "    persona_processed = \"User Persona: \\n\"\n",
    "    for sen in persona:\n",
    "        persona_processed += sen + \"\\n\"\n",
    "\n",
    "    #preprocess the history conversation\n",
    "    history_convo_processed = \"Dialogue history: \\n\"\n",
    "    \n",
    "    #concat all history except the last utter from the bot\n",
    "    for i in range(0,len(history_convo)-1,2):\n",
    "        bot_uttr = \"Bot: \" + history_convo[i]\n",
    "        user_uttr = \"User: \" + history_convo[i+1]\n",
    "        full_uttr = bot_uttr + \"\\n\" + user_uttr + \"\\n\"\n",
    "        history_convo_processed += full_uttr\n",
    "\n",
    "    #add the last utter from bot\n",
    "    history_convo_processed += \"Bot: \" + history_convo[-1] + \"\\n\"\n",
    "\n",
    "    #preprocess the user response\n",
    "    usr_response_processed = \"User: \" + usr_response + \"\\n\"\n",
    "\n",
    "    processed = [persona_processed, history_convo_processed, usr_response_processed]\n",
    "    return unprocessed, processed\n",
    "\n",
    "# materials[0] is persona, materials[1] is history conversation, materials[2] is user response\n",
    "def create_example(dataset, conv_id, implicit=False):\n",
    "    _, materials = extract_format_data(dataset, conv_id)\n",
    "    if implicit:\n",
    "        return materials[1] + materials[2]\n",
    "    else:\n",
    "        return materials[0] + materials[1] + materials[2]\n",
    "    \n",
    "\n",
    "def create_few_shot_examples(dataset, conv_id, few_shot_no, implicit=False):\n",
    "    max_conv_id = dataset['train']['conv_id'][-1]\n",
    "    pool = [num for num in range(1, max_conv_id + 1) if num != conv_id]\n",
    "    random_conv_ids = get_random_conv_id(pool, few_shot_no)\n",
    "\n",
    "    few_shot_examples = \"\"\n",
    "    for index, conv_id in enumerate(random_conv_ids):\n",
    "        few_shot_examples += f\"Demo {index}:\\n\" + create_example(dataset, conv_id, implicit) + \"\\n\"\n",
    "    return few_shot_examples\n",
    "\n",
    "\n",
    "def construct_prompt(dataset, conv_id, prompt_type, few_shot_no=1, print_output= False):\n",
    "    max_conv_id = dataset['train']['conv_id'][-1]\n",
    "    raw_materials, materials = extract_format_data(dataset, conv_id)\n",
    "    raw_target_response = raw_materials[2]\n",
    "    raw_persona_text = raw_materials[0]\n",
    "\n",
    "    system_prompt = \"\"\n",
    "    user_prompt = \"\"\n",
    "    few_shot_examples = \"\"\n",
    "\n",
    "    # only history dialogue\n",
    "    if prompt_type == \"context_only\":\n",
    "        user_prompt += materials[1] + \"User:\"\n",
    "\n",
    "    # task prompt, history dialogue\n",
    "    if prompt_type == \"task_prompt_context_implicit\":\n",
    "        #version 1 Based on the previous conversation history, generate a response for the user that aligns with their profile and the current context of the discussion.\n",
    "        system_prompt += \"Considering the user's profile and the ongoing discussion's context as established in the previous dialogue history, craft a response within 15 words that is coherent, relevant, and tailored to the user's interests and style of communication.\"\n",
    "        user_prompt += materials[1] + \"User:\"\n",
    "\n",
    "    # task prompt, persona, and history dialogue\n",
    "    if prompt_type == \"task_prompt_context_explicit\":\n",
    "        system_prompt += \"Given the user's profile as outlined in the provided persona information, and considering the context of the ongoing discussion from the previous dialogue history, craft a response that is specifically tailored to resonate with the user's explicit characteristics and maintains the continuity of the dialogue.\"\n",
    "        user_prompt += materials[0]+ materials[1] + \"User:\"\n",
    "\n",
    "    # few-shot demos, history dialogue\n",
    "    if prompt_type == \"few_shot_implicit\":\n",
    "        system_prompt += \"Considering the various user profiles and styles depicted in the provided few-shot examples, and the ongoing discussion's context as established in the previous dialogue history, synthesize a coherent and relevant response. This response should be adaptable to the general preferences and communication styles observed in the examples, while seamlessly continuing the dialogue.\"\n",
    "\n",
    "        #set the implicit flag to True to exclude persona in few-shot demos\n",
    "        few_shot_examples = create_few_shot_examples(dataset, conv_id, few_shot_no, implicit=True)\n",
    "        user_prompt += few_shot_examples + materials[1]+ \"User:\"\n",
    "\n",
    "    if print_output:\n",
    "        print(\"### TASK PROMPT ###\\n\" + system_prompt)\n",
    "        print(\"### USER PROMPT ###\\n\" + user_prompt)\n",
    "    \n",
    "\n",
    "    return system_prompt, user_prompt, raw_target_response, raw_persona_text\n",
    "\n",
    "\n",
    "#context_only, task_prompt_context_implicit, task_prompt_context_explicit, few_shot_implicit\n",
    "def test():\n",
    "    dataset = load_dataset(\"bavard/personachat_truecased\", \"full\")\n",
    "    dataset\n",
    "    result = construct_prompt(dataset, 1, \"task_prompt_context_implicit\",print_output=False)\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "    name = \"1\"\n",
    "    name2 = \"2\"\n",
    "\n",
    "    return name, name2\n",
    "\n",
    "result = test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_id = 6\n",
    "dataset = load_dataset(\"bavard/personachat_truecased\", \"full\")\n",
    "#dialog = dataset['validation'].filter(lambda example:example['conv_id']== conv_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['personality', 'candidates', 'history', 'conv_id', 'utterance_idx'],\n",
       "        num_rows: 131438\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['personality', 'candidates', 'history', 'conv_id', 'utterance_idx'],\n",
       "        num_rows: 7801\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper import read_json\n",
    "data = read_json('experiment1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zarius/miniconda3/envs/llm_persona/lib/python3.9/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:11: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Conversation ID': 965,\n",
       " 'BLEU-1': 0,\n",
       " 'BLEU-2': 0,\n",
       " 'BLEU-3': 0,\n",
       " 'BLEU-4': 0,\n",
       " 'ROUGE-1': 0.0,\n",
       " 'ROUGE-2': 0.0,\n",
       " 'ROUGE-L': 0,\n",
       " 'Cosine Similarity': 0.20438892,\n",
       " 'Distinct-1': 0.0,\n",
       " 'Distinct-2': 0.0,\n",
       " 'Token Overlap Ratio': 0.0,\n",
       " 'Character Overlap Ratio': 0.0,\n",
       " 'Inter Similarity': 0.04087778329849243,\n",
       " 'Persona Coverage': 0,\n",
       " 'Persona Recall': 0,\n",
       " 'Persona Precision': 0,\n",
       " 'Persona F1': 0}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from analyze import calculate_metrics\n",
    "calculate_metrics(data[-1]['conv_id'], data[-1]['target_response'], data[-1]['generated_response'], data[-1]['persona_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['Conversation ID', 'BLEU-1', 'BLEU-2', 'BLEU-3', 'BLEU-4', 'ROUGE-1', 'ROUGE-2', 'ROUGE-L', 'Cosine Similarity', 'Distinct-1', 'Distinct-2', 'Token Overlap Ratio', 'Character Overlap Ratio', 'Inter Similarity', 'Persona Coverage', 'Persona Recall', 'Persona Precision', 'Persona F1'])\n"
     ]
    }
   ],
   "source": [
    "metric_keys = data[0].keys()\n",
    "print(metric_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_avg_metrics(data):\n",
    "    avg_metrics = {\n",
    "        'Conversation ID': 0,\n",
    "        'BLEU-1': 0,\n",
    "        'BLEU-2': 0,\n",
    "        'BLEU-3': 0,\n",
    "        'BLEU-4': 0,\n",
    "        'ROUGE-1': 0,\n",
    "        'ROUGE-2': 0,\n",
    "        'ROUGE-L': 0,\n",
    "        'Cosine Similarity': 0,\n",
    "        'Distinct-1': 0,\n",
    "        'Distinct-2': 0,\n",
    "        'Token Overlap Ratio': 0,\n",
    "        'Character Overlap Ratio': 0,\n",
    "        'Inter Similarity': 0,\n",
    "        'Persona Coverage': 0,\n",
    "        'Persona Recall': 0,\n",
    "        'Persona Precision': 0,\n",
    "        'Persona F1': 0\n",
    "    }\n",
    "    for object in data:\n",
    "        for key in object.keys():\n",
    "            avg_metrics[key] += object[key]\n",
    "    \n",
    "    num_objects = len(data)\n",
    "    for key in avg_metrics.keys():\n",
    "        avg_metrics[key] /= num_objects \n",
    "        avg_metrics[key] = avg_metrics[key]*100\n",
    "    return avg_metrics\n",
    "\n",
    "avg_metrics = calculate_avg_metrics(data)\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Conversation ID': 499.5, 'BLEU-1': 0.1232292348429177, 'BLEU-2': 0.049328470627085876, 'BLEU-3': 0.0289745832318289, 'BLEU-4': 0.021344028967990904, 'ROUGE-1': 0.1561807770889836, 'ROUGE-2': 0.02986322457413851, 'ROUGE-L': 0.13987661453868921, 'Cosine Similarity': 0.30141694619180637, 'Distinct-1': 0.9554821806637072, 'Distinct-2': 0.9981878124227666, 'Token Overlap Ratio': 0.09041412052169583, 'Character Overlap Ratio': 0.5681395475299307, 'Inter Similarity': 0.2446152927250076, 'Persona Coverage': 0.0609735787729655, 'Persona Recall': 0.2592147019647016, 'Persona Precision': 0.12054429566079165, 'Persona F1': 0.1550908013546865}\n"
     ]
    }
   ],
   "source": [
    "from analyze import print_avg_metrics\n",
    "print_avg_metrics(\"experiment1_metrics\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hi how are you?',\n",
       " 'I am doing great on this crisp fall night.',\n",
       " 'Do you like sports? I love tennis and play it for a living.',\n",
       " 'Sports are ok but I love cooking shows and cooking.',\n",
       " 'My husband is a doctor so I cook meals and freeze them to have on hand.',\n",
       " 'That is a great idea I am single so I only make enough for one.',\n",
       " 'Well enjoy being single, we are growing our family so free time is short.',\n",
       " 'But it is tough cause I am vegan and my friends are not.',\n",
       " 'Bummer, I take my dog for long walks to De stress, how about you?',\n",
       " 'I like to go skydiving or ride a rollercoaster.',\n",
       " 'When its raining or too cold we walk the stairs in our home, three stories!',\n",
       " 'Wow first floor apt for me so no stairs.',\n",
       " 'With a big family we need the room, plus the dog.',\n",
       " 'How many children do u have.',\n",
       " 'Three and one on the way. The dog is like a child too.']"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['validation']['history'][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading GloVe model...\n",
      "Persona Coverage: 0.08407729872580033\n",
      "{'the', 'on', 'cat', 'sat', 'mat'}\n",
      "{'the', 'on', 'cat', 'sat', 'mat'}\n",
      "Persona Recall: 0.6\n",
      "Persona Precision: 0.6\n",
      "P-F1: 0.6\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import gensim.downloader as api\n",
    "import spacy\n",
    "tokenizer = spacy.load('en_core_web_sm')\n",
    "\n",
    "def tokenize(text):\n",
    "    doc = tokenizer(text)\n",
    "    # Filter out punctuation and stop words\n",
    "    return [token.text.lower() for token in doc if token.is_alpha or token.is_digit]\n",
    "\n",
    "def calculate_idf(tf_j):\n",
    "    \"\"\"Calculate the inverse document frequency (idf) for a given term frequency (tf_j).\"\"\"\n",
    "    return 1 / (1 + math.log(1 + tf_j))\n",
    "\n",
    "def calculate_tf(idx):\n",
    "    \"\"\"Calculate the term frequency (tf) using Zipf's Law for a Glove-twitter-100 index (idx).\"\"\"\n",
    "    return 1e6 * (1 / (idx ** 1.07))\n",
    "\n",
    "def calculate_persona_coverage(response, personas, glove_model):\n",
    "    \"\"\"\n",
    "    Calculate Persona Coverage.\n",
    "    \n",
    "    Args:\n",
    "    response (str): The generated response sentence.\n",
    "    personas (list): A list of persona sentences.\n",
    "    glove_model: Pre-trained GloVe model from gensim.\n",
    "    \n",
    "    Returns:\n",
    "    float: The Persona Coverage score.\n",
    "    \"\"\"\n",
    "    # Tokenize the response and persona sentences\n",
    "    W_Y = set(response.split())\n",
    "    \n",
    "    # Initialize the maximum persona coverage score\n",
    "    max_p_cover = 0\n",
    "    \n",
    "    # Iterate over each persona sentence\n",
    "    for persona in personas:\n",
    "        W_p_i = set(persona.split())\n",
    "        \n",
    "        # Calculate the intersection of words between response and persona\n",
    "        intersection = W_Y.intersection(W_p_i)\n",
    "        \n",
    "        if not intersection:\n",
    "            continue\n",
    "        \n",
    "        # Calculate the sum of idf values for words in the intersection\n",
    "        sum_idf = 0\n",
    "        for word in intersection:\n",
    "            if word in glove_model.key_to_index:\n",
    "                idx = glove_model.key_to_index[word] + 1  # GloVe index is 0-based, so add 1\n",
    "                tf = calculate_tf(idx)\n",
    "                idf = calculate_idf(tf)\n",
    "                sum_idf += idf\n",
    "        \n",
    "        # Normalize by the size of the intersection\n",
    "        p_cover = sum_idf / len(intersection)\n",
    "        \n",
    "        # Update the maximum persona coverage score\n",
    "        if p_cover > max_p_cover:\n",
    "            max_p_cover = p_cover\n",
    "    \n",
    "    return max_p_cover\n",
    "\n",
    "def calculate_persona_recall(response, personas):\n",
    "    \"\"\"\n",
    "    Calculate Persona Recall.\n",
    "    \n",
    "    Args:\n",
    "    response (str): The generated response sentence.\n",
    "    personas (list): A list of persona sentences.\n",
    "    \n",
    "    Returns:\n",
    "    float: The Persona Recall score.\n",
    "    \"\"\"\n",
    "    W_Y = set(tokenize(response))\n",
    "    max_recall = 0\n",
    "    print(W_Y)\n",
    "    for persona in personas:\n",
    "        W_p_i = set(tokenize(persona))\n",
    "        intersection = W_Y & W_p_i\n",
    "        recall = len(intersection) / len(W_p_i)\n",
    "        if recall > max_recall:\n",
    "            max_recall = recall\n",
    "\n",
    "    return max_recall\n",
    "\n",
    "def calculate_persona_precision(response, personas):\n",
    "    \"\"\"\n",
    "    Calculate Persona Precision.\n",
    "    \n",
    "    Args:\n",
    "    response (str): The generated response sentence.\n",
    "    personas (list): A list of persona sentences.\n",
    "    \n",
    "    Returns:\n",
    "    float: The Persona Precision score.\n",
    "    \"\"\"\n",
    "    W_Y = set(tokenize(response))\n",
    "    max_precision = 0\n",
    "\n",
    "    for persona in personas:\n",
    "        W_p_i = set(tokenize(persona))\n",
    "        intersection = W_Y & W_p_i\n",
    "        precision = len(intersection) / len(W_Y)\n",
    "        if precision > max_precision:\n",
    "            max_precision = precision\n",
    "\n",
    "    return max_precision\n",
    "\n",
    "def calculate_p_f1(response, personas):\n",
    "    \"\"\"\n",
    "    Calculate P-F1.\n",
    "    \n",
    "    Args:\n",
    "    response (str): The generated response sentence.\n",
    "    personas (list): A list of persona sentences.\n",
    "    \n",
    "    Returns:\n",
    "    float: The P-F1 score.\n",
    "    \"\"\"\n",
    "    recall = calculate_persona_recall(response, personas)\n",
    "    precision = calculate_persona_precision(response, personas)\n",
    "    if recall + precision == 0:\n",
    "        return 0\n",
    "    p_f1 = 2 * recall * precision / (recall + precision)\n",
    "    return p_f1\n",
    "\n",
    "# Load the GloVe model from gensim\n",
    "print(\"Loading GloVe model...\")\n",
    "glove_model = api.load(\"glove-twitter-100\")  # You can choose other dimensions, e.g., 50, 200, 300\n",
    "\n",
    "# Example usage:\n",
    "response = \"The cat sat on the mat\"\n",
    "personas = [\n",
    "    \"The cat is happy\",\n",
    "    \"The dog sat on the log\",\n",
    "    \"The cat chased the dog\"\n",
    "]\n",
    "\n",
    "# Calculate Persona Coverage\n",
    "persona_coverage = calculate_persona_coverage(response, personas, glove_model)\n",
    "print(\"Persona Coverage:\", persona_coverage)\n",
    "\n",
    "# Calculate Persona Recall, Precision, and P-F1\n",
    "persona_recall = calculate_persona_recall(response, personas)\n",
    "persona_precision = calculate_persona_precision(response, personas)\n",
    "p_f1 = calculate_p_f1(response, personas)\n",
    "\n",
    "print(\"Persona Recall:\", persona_recall)\n",
    "print(\"Persona Precision:\", persona_precision)\n",
    "print(\"P-F1:\", p_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The', 'cat', 'sat', 'on', 'the', 'mat!']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = \"The cat sat on the mat!\"\n",
    "response.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My mom is my best friend.\n"
     ]
    }
   ],
   "source": [
    "print(result[3][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "values = range(1,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "for value in range(1,2):\n",
    "    print(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Individual 1-gram: 0.750000\n",
      "Individual 2-gram: 0.500000\n",
      "Individual 3-gram: 0.000000\n",
      "Individual 4-gram: 0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zarius/miniconda3/envs/llm_persona/lib/python3.9/site-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/Users/zarius/miniconda3/envs/llm_persona/lib/python3.9/site-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# n-gram individual BLEU\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "reference = [['this', 'is', 'one', 'test']]\n",
    "candidate = ['this', 'is', 'a', 'test']\n",
    "print('Individual 1-gram: %f' % sentence_bleu(reference, candidate, weights=(1, 0, 0, 0)))\n",
    "print('Individual 2-gram: %f' % sentence_bleu(reference, candidate, weights=(0.5, 0.5, 0, 0)))\n",
    "print('Individual 3-gram: %f' % sentence_bleu(reference, candidate, weights=(0, 0, 1, 0)))\n",
    "print('Individual 4-gram: %f' % sentence_bleu(reference, candidate, weights=(0, 0, 0, 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "def tokenize(text):\n",
    "    \"\"\" Tokenizes the input text using spaCy and returns a list of token texts. \"\"\"\n",
    "    doc = nlp(text)\n",
    "    return [token.text for token in doc]\n",
    "\n",
    "tokens = tokenize(\"Hello, world! How are you doing today?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens\n",
    "sentence = \"Hello, world! How are you doing today?\"\n",
    "result = [sentence.split()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x800 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Sample data in the specified dictionary format\n",
    "data = [\n",
    "    {\"conv_id\": \"001\", \"attempt_no\": 1, \"generated_response\": \"Hello\", \"target_response\": \"Hi\", \"metrics\": 0.75},\n",
    "    {\"conv_id\": \"001\", \"attempt_no\": 2, \"generated_response\": \"How are you?\", \"target_response\": \"How's it going?\", \"metrics\": 0.65},\n",
    "    {\"conv_id\": \"002\", \"attempt_no\": 1, \"generated_response\": \"I'm fine\", \"target_response\": \"I am good\", \"metrics\": 0.80},\n",
    "    {\"conv_id\": \"002\", \"attempt_no\": 2, \"generated_response\": \"What's up?\", \"target_response\": \"What is happening?\", \"metrics\": 0.70}\n",
    "]\n",
    "\n",
    "# Convert list of dictionaries to DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Pivot the DataFrame correctly using keyword arguments\n",
    "heatmap_data = df.pivot(index=\"conv_id\", columns=\"attempt_no\", values=\"metrics\")\n",
    "\n",
    "# Set up the matplotlib figure\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "# Draw the heatmap\n",
    "sns.heatmap(heatmap_data, annot=True, cmap='Blues', linewidths=.5)\n",
    "\n",
    "# Add labels and title if necessary\n",
    "plt.title('Heatmap of Metrics by Conversation ID and Attempt Number')\n",
    "plt.xlabel('Attempt Number')\n",
    "plt.ylabel('Conversation ID')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>conv_id</th>\n",
       "      <th>attempt_no</th>\n",
       "      <th>generated_response</th>\n",
       "      <th>target_response</th>\n",
       "      <th>metrics</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>001</td>\n",
       "      <td>1</td>\n",
       "      <td>Hello</td>\n",
       "      <td>Hi</td>\n",
       "      <td>0.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>001</td>\n",
       "      <td>2</td>\n",
       "      <td>How are you?</td>\n",
       "      <td>How's it going?</td>\n",
       "      <td>0.65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>002</td>\n",
       "      <td>1</td>\n",
       "      <td>I'm fine</td>\n",
       "      <td>I am good</td>\n",
       "      <td>0.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>002</td>\n",
       "      <td>2</td>\n",
       "      <td>What's up?</td>\n",
       "      <td>What is happening?</td>\n",
       "      <td>0.70</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  conv_id  attempt_no generated_response     target_response  metrics\n",
       "0     001           1              Hello                  Hi     0.75\n",
       "1     001           2       How are you?     How's it going?     0.65\n",
       "2     002           1           I'm fine           I am good     0.80\n",
       "3     002           2         What's up?  What is happening?     0.70"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "sentence1 = \"Hello\"\n",
    "sentence2 = \"Hi\"\n",
    "model_name='all-MiniLM-L6-v2'\n",
    "model = SentenceTransformer(model_name)\n",
    "embeddings = model.encode([sentence1, sentence2])\n",
    "cos_sim = cosine_similarity([embeddings[0]], [embeddings[1]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(384,)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity_embeddings(sentence1, sentence2, model_name='all-MiniLM-L6-v2'):\n",
    "    model = SentenceTransformer(model_name)\n",
    "    embeddings = model.encode([sentence1, sentence2])\n",
    "    cos_sim = cosine_similarity([embeddings[0]], [embeddings[1]])[0][0]\n",
    "    return cos_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "If I have time outside of hunting and remodeling homes. Which is not much!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|██████████| 131438/131438 [00:02<00:00, 47802.71 examples/s]\n",
      "Filter: 100%|██████████| 7801/7801 [00:00<00:00, 44212.25 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm also fascinated with mermaids.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|██████████| 131438/131438 [00:02<00:00, 57902.15 examples/s]\n",
      "Filter: 100%|██████████| 7801/7801 [00:00<00:00, 51856.29 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Those are really yummy too, but not my favorite.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|██████████| 131438/131438 [00:02<00:00, 60450.45 examples/s]\n",
      "Filter: 100%|██████████| 7801/7801 [00:00<00:00, 58207.69 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I doubt that very much. You probably like to scream alone.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|██████████| 131438/131438 [00:02<00:00, 61108.09 examples/s]\n",
      "Filter: 100%|██████████| 7801/7801 [00:00<00:00, 57068.30 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That is awesome. Do you like it?\n"
     ]
    }
   ],
   "source": [
    "\n",
    "dataset = load_dataset(\"bavard/personachat_truecased\", \"full\")\n",
    "cos_sim_list = []\n",
    "for conv_id in range(0, 5):\n",
    "    dialog = dataset.filter(lambda example:example['conv_id']== conv_id)\n",
    "    persona = dialog['train']['personality'][-1]\n",
    "    history_convo = dialog['train']['history'][-1]\n",
    "    usr_response = dialog['train']['candidates'][-1][-1]\n",
    "    print (usr_response)\n",
    "    cos_sim = cosine_similarity_embeddings(usr_response,history_convo[-1])\n",
    "    cos_sim_list.append(cos_sim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.14119357, 0.3144738, 0.427917, 0.27486795, 0.18668725]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cos_sim_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You must be very fast. Hunting is one of my favorite hobbies.\n",
      "I am! For my hobby I like to do canning or some whittling.\n",
      "I also remodel homes when I am not out bow hunting.\n",
      "That's neat. When I was in high school I placed 6th in 100m dash!\n",
      "That's awesome. Do you have a favorite season or time of year?\n",
      "I do not. But I do have a favorite meat since that is all I eat exclusively.\n",
      "What is your favorite meat to eat?\n",
      "I would have to say its prime rib. Do you have any favorite foods?\n",
      "I like chicken or macaroni and cheese.\n",
      "Do you have anything planned for today? I think I am going to do some canning.\n",
      "I am going to watch football. What are you canning?\n",
      "I think I will can some jam. Do you also play footfall for fun?\n"
     ]
    }
   ],
   "source": [
    "### New metric to measure how likely the user is willing to start new topic\n",
    "# start from 0 to indicate bot as first, start from 1 to indicate user as first\n",
    "# 0 indicates how user \n",
    "cos_sim_list = []\n",
    "for i in range(1,len(history_convo)-1,2):\n",
    "    bot_uttr = history_convo[i]\n",
    "    user_uttr =  history_convo[i+1]\n",
    "    cos_sim = cosine_similarity_embeddings(bot_uttr, user_uttr)\n",
    "    cos_sim_list.append(cos_sim)\n",
    "    print(bot_uttr)\n",
    "    print(user_uttr)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.42948577801386517\n"
     ]
    }
   ],
   "source": [
    "# user's willingness \n",
    "cos_sim_list\n",
    "avg_willingness = sum(cos_sim_list)/len(cos_sim_list)\n",
    "print(avg_willingness)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.30346233, 0.018240921, 0.3325431, 0.73453104, 0.25507307, 0.23173182]\n",
      "0.31259704753756523\n"
     ]
    }
   ],
   "source": [
    "# bot's willingness\n",
    "print(cos_sim_list)\n",
    "avg_willingness = sum(cos_sim_list)/len(cos_sim_list)\n",
    "print(avg_willingness)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic drift detected at sentence indices: [1, 2]\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Load pre-trained BERT model and tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Function to get BERT embeddings for a sentence\n",
    "def get_embedding(sentence):\n",
    "    inputs = tokenizer(sentence, return_tensors='pt')\n",
    "    outputs = model(**inputs)\n",
    "    return outputs.last_hidden_state.mean(dim=1).detach().numpy()\n",
    "\n",
    "# Example conversation\n",
    "conversation = [\n",
    "    \"Let's talk about the recent advancements in AI.\",\n",
    "    \"Sure, AI has made significant progress in various fields.\",\n",
    "    \"What do you think about machine learning?\",\n",
    "    \"Machine learning is a subset of AI that focuses on algorithms.\"\n",
    "]\n",
    "\n",
    "# Compute embeddings for each sentence\n",
    "embeddings = [get_embedding(sentence) for sentence in conversation]\n",
    "\n",
    "# Measure semantic similarity between consecutive sentences\n",
    "similarities = [cosine_similarity(embeddings[i], embeddings[i+1])[0][0] for i in range(len(embeddings)-1)]\n",
    "\n",
    "# Define a threshold to detect topic drift (e.g., similarity below 0.7 indicates drift)\n",
    "threshold = 0.7\n",
    "topic_drift_points = [i for i, similarity in enumerate(similarities) if similarity < threshold]\n",
    "\n",
    "print(\"Topic drift detected at sentence indices:\", topic_drift_points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.7349808, 0.6413611, 0.6749538]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similarities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cornel Movie Dialogs Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading builder script: 100%|██████████| 7.11k/7.11k [00:00<00:00, 14.3MB/s]\n",
      "Downloading readme: 100%|██████████| 7.35k/7.35k [00:00<00:00, 18.8MB/s]\n",
      "Downloading data: 100%|██████████| 9.92M/9.92M [00:01<00:00, 6.16MB/s]\n",
      "Generating train split: 100%|██████████| 83097/83097 [1:08:03<00:00, 20.35 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"cornell-movie-dialog/cornell_movie_dialog\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'movieID': ' m0 ',\n",
       " 'movieTitle': ' 10 things i hate about you ',\n",
       " 'movieYear': ' 1999 ',\n",
       " 'movieIMDBRating': ' 6.90 ',\n",
       " 'movieNoIMDBVotes': ' 62847 ',\n",
       " 'movieGenres': ['comedy', 'romance'],\n",
       " 'characterID1': 'u0 ',\n",
       " 'characterID2': ' u2 ',\n",
       " 'characterName1': ' BIANCA ',\n",
       " 'characterName2': ' CAMERON ',\n",
       " 'utterance': {'text': ['L207 ', 'L208 '], 'LineID': ['L207', 'L208']}}"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'][4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from convokit import Corpus, download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading movie-corpus to /Users/zarius/.convokit/downloads/movie-corpus\n",
      "Downloading movie-corpus from http://zissou.infosci.cornell.edu/convokit/datasets/movie-corpus/movie-corpus.zip (40.9MB)... Done\n"
     ]
    }
   ],
   "source": [
    "corpus = Corpus(filename=download(\"movie-corpus\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L637568\n",
      "14\n",
      "L552628\n",
      "14\n",
      "L406442\n",
      "16\n",
      "L221323\n",
      "16\n",
      "L357008\n",
      "14\n",
      "L396440\n",
      "12\n",
      "L267935\n",
      "14\n",
      "L484337\n",
      "12\n",
      "L147732\n",
      "12\n",
      "L356806\n",
      "12\n",
      "        timestamp                                   text speaker reply_to  \\\n",
      "id                                                                          \n",
      "L356817      None  Zoe, come say hello to your father...   u6384  L356816   \n",
      "L356816      None                                  Yeah.   u6378  L356815   \n",
      "L356815      None           Do you want to talk to  Zoe?   u6384  L356814   \n",
      "L356814      None                                  What?   u6378  L356813   \n",
      "L356813      None                        Do you want to-   u6384  L356812   \n",
      "L356812      None                               It's ok.   u6378  L356811   \n",
      "L356811      None                  Fine. Is it going ok?   u6384  L356810   \n",
      "L356810      None      I couldn't hear you. How are you?   u6378  L356809   \n",
      "L356809      None                 It's Lydia, your wife.   u6384  L356808   \n",
      "L356808      None                                 Hello?   u6378  L356807   \n",
      "L356807      None                   Hello, Bob, it's me.   u6384  L356806   \n",
      "L356806      None                                 Hello?   u6378     None   \n",
      "\n",
      "        conversation_id meta.movie_id  \\\n",
      "id                                      \n",
      "L356817         L356806          m425   \n",
      "L356816         L356806          m425   \n",
      "L356815         L356806          m425   \n",
      "L356814         L356806          m425   \n",
      "L356813         L356806          m425   \n",
      "L356812         L356806          m425   \n",
      "L356811         L356806          m425   \n",
      "L356810         L356806          m425   \n",
      "L356809         L356806          m425   \n",
      "L356808         L356806          m425   \n",
      "L356807         L356806          m425   \n",
      "L356806         L356806          m425   \n",
      "\n",
      "                                               meta.parsed vectors  \n",
      "id                                                                  \n",
      "L356817  [{'rt': 2, 'toks': [{'tok': 'Zoe', 'tag': 'NNP...      []  \n",
      "L356816  [{'rt': 0, 'toks': [{'tok': 'Yeah', 'tag': 'UH...      []  \n",
      "L356815  [{'rt': 2, 'toks': [{'tok': 'Do', 'tag': 'VBP'...      []  \n",
      "L356814  [{'rt': 0, 'toks': [{'tok': 'What', 'tag': 'WP...      []  \n",
      "L356813  [{'rt': 2, 'toks': [{'tok': 'Do', 'tag': 'VBP'...      []  \n",
      "L356812  [{'rt': 1, 'toks': [{'tok': 'It', 'tag': 'PRP'...      []  \n",
      "L356811  [{'rt': 0, 'toks': [{'tok': 'Fine', 'tag': 'UH...      []  \n",
      "L356810  [{'rt': 3, 'toks': [{'tok': 'I', 'tag': 'PRP',...      []  \n",
      "L356809  [{'rt': 1, 'toks': [{'tok': 'It', 'tag': 'PRP'...      []  \n",
      "L356808  [{'rt': 0, 'toks': [{'tok': 'Hello', 'tag': 'U...      []  \n",
      "L356807  [{'rt': 5, 'toks': [{'tok': 'Hello', 'tag': 'U...      []  \n",
      "L356806  [{'rt': 0, 'toks': [{'tok': 'Hello', 'tag': 'U...      []  \n"
     ]
    }
   ],
   "source": [
    "num_of_utterances = 0\n",
    "num_of_speakers = 0\n",
    "conv_ids = []\n",
    "conv_id = 0\n",
    " # only select conversation with 2 speakers and 12, 14, 16 utterances\n",
    "\n",
    "while len(conv_ids) < 10:\n",
    "    df = corpus.random_conversation().get_utterances_dataframe()\n",
    "    num_of_utterances = df.shape[0]\n",
    "    num_of_speakers = df['speaker'].nunique()\n",
    "    conv_id = df['conversation_id'].iloc[0]\n",
    "    #print(\"here\")\n",
    "    if ((num_of_utterances == 16 or num_of_utterances == 14 or num_of_utterances == 12) and (num_of_speakers == 2) and conv_id not in conv_ids):\n",
    "        conv_ids.append(conv_id)\n",
    "        print(conv_id)\n",
    "        print(num_of_utterances)\n",
    "    \n",
    "\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['L637568', 'L552628', 'L406442', 'L221323', 'L357008', 'L396440', 'L267935', 'L484337', 'L147732', 'L356806']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(conv_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The list has been added to config.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Example list\n",
    "my_list = conv_ids\n",
    "\n",
    "# Specify the filename\n",
    "filename = 'config.json'\n",
    "\n",
    "# Read the existing JSON file\n",
    "with open(filename, 'r') as file:\n",
    "    config_data = json.load(file)\n",
    "\n",
    "# Update the content with the new list\n",
    "config_data['conv_ids'] = my_list\n",
    "\n",
    "# Save the updated content back to the JSON file\n",
    "with open(filename, 'w') as file:\n",
    "    json.dump(config_data, file, indent=4)\n",
    "\n",
    "print(f\"The list has been added to {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The list loaded from config.json: ['L637568', 'L552628', 'L406442', 'L221323', 'L357008', 'L396440', 'L267935', 'L484337', 'L147732', 'L356806']\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Specify the filename\n",
    "filename = 'config.json'\n",
    "\n",
    "# Load the updated content from the JSON file\n",
    "with open(filename, 'r') as file:\n",
    "    config_data = json.load(file)\n",
    "\n",
    "# Access the list\n",
    "my_list = config_data['conv_ids']\n",
    "# my_list = config_data.get('conv_ids', [])\n",
    "print(f\"The list loaded from {filename}: {my_list}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = corpus.get_conversation(my_list[1]).get_utterances_dataframe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reverse the order\n",
    "reversed_df = df.iloc[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(reversed_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_convo = reversed_df['text'].iloc[:-1].tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(history_convo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_convo_processed = \"Dialogue history: \\n\"\n",
    "\n",
    "for i in range(0,len(history_convo)-1,2):\n",
    "    bot_uttr = \"Bot: \" + history_convo[i]\n",
    "    user_uttr = \"User: \" + history_convo[i+1]\n",
    "    full_uttr = bot_uttr + \"\\n\" + user_uttr + \"\\n\"\n",
    "    history_convo_processed += full_uttr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dialogue history: \n",
      "Bot: Do you have any witnesses, Major?\n",
      "User: What?\n",
      "Bot: What happened, Major?  Some kind of private beef between the two of you?\n",
      "User: Look, I didn't shoot him!\n",
      "Bot: We'll have to leave that up to the C.I.D. boys, won't we, Major?\n",
      "User: Look, Colonel.  I've got to talk to somebody at Air Command.\n",
      "Bot: Don't worry, Major.  Your rights will be fully protected.\n",
      "User: Colonel, don't you know what's going on?\n",
      "Bot: Sure I do.  There was some kind of mutiny on the base, and you killed General Ripper.\n",
      "User: Look, General Ripper went off his rocker and ordered the 843rd Bomb Wing to attack with H-bombs.\n",
      "Bot: You must think I'm an awful sap, Major.  Just sit down, fella, and keep your hands on the desk!\n",
      "User: Didn't they tell you?\n",
      "Bot: They told me, Major.  And I didn't hear anything about any atomic attack.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#add the last utter from bot\n",
    "history_convo_processed += \"Bot: \" + history_convo[-1] + \"\\n\"\n",
    "print(history_convo_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "usr_response = reversed_df['text'].iloc[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Look, Colonel.  You keep me covered, but let me just pick up this red telephone that connects to Air Command headquarters. Okay?...I won't play any tricks on you... Okay?\""
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "usr_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocess the user response\n",
    "usr_response_processed = \"User: \" + usr_response + \"\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = list(corpus.iter_speakers())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L860\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>text</th>\n",
       "      <th>speaker</th>\n",
       "      <th>reply_to</th>\n",
       "      <th>conversation_id</th>\n",
       "      <th>meta.movie_id</th>\n",
       "      <th>meta.parsed</th>\n",
       "      <th>vectors</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>L861</th>\n",
       "      <td>None</td>\n",
       "      <td>No...</td>\n",
       "      <td>u2</td>\n",
       "      <td>L860</td>\n",
       "      <td>L860</td>\n",
       "      <td>m0</td>\n",
       "      <td>[{'rt': 0, 'toks': [{'tok': 'No', 'tag': 'UH',...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>L860</th>\n",
       "      <td>None</td>\n",
       "      <td>Then Guillermo says, \"If you go any lighter, y...</td>\n",
       "      <td>u0</td>\n",
       "      <td>None</td>\n",
       "      <td>L860</td>\n",
       "      <td>m0</td>\n",
       "      <td>[{'rt': 2, 'toks': [{'tok': 'Then', 'tag': 'RB...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     timestamp                                               text speaker  \\\n",
       "id                                                                          \n",
       "L861      None                                              No...      u2   \n",
       "L860      None  Then Guillermo says, \"If you go any lighter, y...      u0   \n",
       "\n",
       "     reply_to conversation_id meta.movie_id  \\\n",
       "id                                            \n",
       "L861     L860            L860            m0   \n",
       "L860     None            L860            m0   \n",
       "\n",
       "                                            meta.parsed vectors  \n",
       "id                                                               \n",
       "L861  [{'rt': 0, 'toks': [{'tok': 'No', 'tag': 'UH',...      []  \n",
       "L860  [{'rt': 2, 'toks': [{'tok': 'Then', 'tag': 'RB...      []  "
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv_id = data[6]\n",
    "print(conv_id)\n",
    "convo = corpus.get_conversation(conv_id)\n",
    "convo.get_utterances_dataframe().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "uttr = corpus.random_utterance()\n",
    "convo = uttr.get_conversation()\n",
    "spkr = uttr.speaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'7yy032'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[79], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m convo \u001b[38;5;241m=\u001b[39m \u001b[43mcorpus\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_conversation\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m7yy032\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/llm_persona/lib/python3.9/site-packages/convokit/model/corpus.py:355\u001b[0m, in \u001b[0;36mCorpus.get_conversation\u001b[0;34m(self, convo_id)\u001b[0m\n\u001b[1;32m    348\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_conversation\u001b[39m(\u001b[38;5;28mself\u001b[39m, convo_id: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Conversation:\n\u001b[1;32m    349\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    350\u001b[0m \u001b[38;5;124;03m    Gets Conversation of the specified id from the corpus\u001b[39;00m\n\u001b[1;32m    351\u001b[0m \n\u001b[1;32m    352\u001b[0m \u001b[38;5;124;03m    :param convo_id: id of Conversation\u001b[39;00m\n\u001b[1;32m    353\u001b[0m \u001b[38;5;124;03m    :return: Conversation\u001b[39;00m\n\u001b[1;32m    354\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 355\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconversations\u001b[49m\u001b[43m[\u001b[49m\u001b[43mconvo_id\u001b[49m\u001b[43m]\u001b[49m\n",
      "\u001b[0;31mKeyError\u001b[0m: '7yy032'"
     ]
    }
   ],
   "source": [
    "convo = corpus.get_conversation('7yy032')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>text</th>\n",
       "      <th>speaker</th>\n",
       "      <th>reply_to</th>\n",
       "      <th>conversation_id</th>\n",
       "      <th>meta.movie_id</th>\n",
       "      <th>meta.parsed</th>\n",
       "      <th>vectors</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>L450786</th>\n",
       "      <td>None</td>\n",
       "      <td>What?</td>\n",
       "      <td>u7142</td>\n",
       "      <td>L450785</td>\n",
       "      <td>L450785</td>\n",
       "      <td>m479</td>\n",
       "      <td>[{'rt': 0, 'toks': [{'tok': 'What', 'tag': 'WP...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>L450785</th>\n",
       "      <td>None</td>\n",
       "      <td>Housekeeping.  I come to turn down the bed.  5...</td>\n",
       "      <td>u7138</td>\n",
       "      <td>None</td>\n",
       "      <td>L450785</td>\n",
       "      <td>m479</td>\n",
       "      <td>[{'rt': 0, 'toks': [{'tok': 'Housekeeping', 't...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        timestamp                                               text speaker  \\\n",
       "id                                                                             \n",
       "L450786      None                                              What?   u7142   \n",
       "L450785      None  Housekeeping.  I come to turn down the bed.  5...   u7138   \n",
       "\n",
       "        reply_to conversation_id meta.movie_id  \\\n",
       "id                                               \n",
       "L450786  L450785         L450785          m479   \n",
       "L450785     None         L450785          m479   \n",
       "\n",
       "                                               meta.parsed vectors  \n",
       "id                                                                  \n",
       "L450786  [{'rt': 0, 'toks': [{'tok': 'What', 'tag': 'WP...      []  \n",
       "L450785  [{'rt': 0, 'toks': [{'tok': 'Housekeeping', 't...      []  "
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convo.get_utterances_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "spkr_convos = list(spkr.iter_conversations())\n",
    "spkr_utts = list(spkr.iter_utterances())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "'<' not supported between instances of 'NoneType' and 'NoneType'\nUtterance timestamps may not have been set correctly.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m~/miniconda3/envs/llm_persona/lib/python3.9/site-packages/convokit/model/conversation.py:413\u001b[0m, in \u001b[0;36mConversation.get_chronological_utterance_list\u001b[0;34m(self, selector)\u001b[0m\n\u001b[1;32m    412\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 413\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msorted\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    414\u001b[0m \u001b[43m        \u001b[49m\u001b[43m[\u001b[49m\u001b[43mutt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mutt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miter_utterances\u001b[49m\u001b[43m(\u001b[49m\u001b[43mselector\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mutt\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mutt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimestamp\u001b[49m\n\u001b[1;32m    415\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    416\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[0;31mTypeError\u001b[0m: '<' not supported between instances of 'NoneType' and 'NoneType'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[91], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mconvo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_chronological_utterance_list\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/llm_persona/lib/python3.9/site-packages/convokit/model/conversation.py:417\u001b[0m, in \u001b[0;36mConversation.get_chronological_utterance_list\u001b[0;34m(self, selector)\u001b[0m\n\u001b[1;32m    413\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msorted\u001b[39m(\n\u001b[1;32m    414\u001b[0m         [utt \u001b[38;5;28;01mfor\u001b[39;00m utt \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miter_utterances(selector)], key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m utt: utt\u001b[38;5;241m.\u001b[39mtimestamp\n\u001b[1;32m    415\u001b[0m     )\n\u001b[1;32m    416\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 417\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;28mstr\u001b[39m(e) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mUtterance timestamps may not have been set correctly.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: '<' not supported between instances of 'NoneType' and 'NoneType'\nUtterance timestamps may not have been set correctly."
     ]
    }
   ],
   "source": [
    "convo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_format_data_movie(movie_df, conv_id, section='train'):\n",
    "    dialog = dataset[section].filter(lambda example:example['conv_id']== conv_id)\n",
    "    persona = dialog['personality'][-1]\n",
    "    history_convo = dialog['history'][-1]\n",
    "    usr_response = dialog['candidates'][-1][-1]\n",
    "\n",
    "    unprocessed = [persona, history_convo, usr_response]\n",
    "\n",
    "    #preprocess the persona\n",
    "    persona_processed = \"User Persona: \\n\"\n",
    "    for sen in persona:\n",
    "        persona_processed += sen + \"\\n\"\n",
    "\n",
    "    #preprocess the history conversation\n",
    "    history_convo_processed = \"Dialogue history: \\n\"\n",
    "    \n",
    "    #concat all history except the last utter from the bot\n",
    "    for i in range(0,len(history_convo)-1,2):\n",
    "        bot_uttr = \"Bot: \" + history_convo[i]\n",
    "        user_uttr = \"User: \" + history_convo[i+1]\n",
    "        full_uttr = bot_uttr + \"\\n\" + user_uttr + \"\\n\"\n",
    "        history_convo_processed += full_uttr\n",
    "\n",
    "    #add the last utter from bot\n",
    "    history_convo_processed += \"Bot: \" + history_convo[-1] + \"\\n\"\n",
    "\n",
    "    #preprocess the user response\n",
    "    usr_response_processed = \"User: \" + usr_response + \"\\n\"\n",
    "\n",
    "    processed = [persona_processed, history_convo_processed, usr_response_processed]\n",
    "    return unprocessed, processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'slice'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# List the first few utterances\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m utt \u001b[38;5;129;01min\u001b[39;00m \u001b[43mcorpus\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mutterances\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m]\u001b[49m:\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28mprint\u001b[39m(utt\u001b[38;5;241m.\u001b[39mtext)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# List the first few speakers\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: unhashable type: 'slice'"
     ]
    }
   ],
   "source": [
    "# List the first few utterances\n",
    "for utt in corpus.utterances[:5]:\n",
    "    print(utt.text)\n",
    "\n",
    "# List the first few speakers\n",
    "for spk in list(corpus.speakers.keys())[:5]:\n",
    "    print(spk)\n",
    "\n",
    "# List the first few conversations\n",
    "for convo in list(corpus.conversations.keys())[:5]:\n",
    "    print(convo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading readme: 100%|██████████| 4.75k/4.75k [00:00<00:00, 15.1MB/s]\n",
      "Downloading data: 100%|██████████| 14.6M/14.6M [00:01<00:00, 8.20MB/s]\n",
      "Generating movie_lines split: 100%|██████████| 304713/304713 [00:00<00:00, 1991915.74 examples/s]\n"
     ]
    }
   ],
   "source": [
    "dataset2 = load_dataset(\"spawn99/CornellMovieDialogCorpus\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    movie_lines: Dataset({\n",
       "        features: ['lineID', 'characterID', 'movieID', 'characterName', 'utterance'],\n",
       "        num_rows: 304713\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = dataset2['movie_lines'].filter(lambda example:example['movieID']== \"m0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'lineID': 'L1044',\n",
       " 'characterID': 'u2',\n",
       " 'movieID': 'm0',\n",
       " 'characterName': 'CAMERON',\n",
       " 'utterance': 'They do to!'}"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zarius/miniconda3/envs/llm_persona/lib/python3.9/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:11: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Conversation ID': 0, 'BLEU-1': 7.934612888501345, 'BLEU-2': 2.3656714136578905, 'BLEU-3': 1.3448370182769471, 'BLEU-4': 0.9347682646059561, 'ROUGE-1': 12.317371516387734, 'ROUGE-2': 1.5091142974381293, 'ROUGE-L': 9.986054508155949, 'Cosine Similarity': 19.568868968635798, 'Distinct-1': 90.73398072519451, 'Distinct-2': 98.83592592592592, 'Token Overlap Ratio': 6.785841634511783, 'Character Overlap Ratio': 51.82811138274294, 'Inter Similarity': 19.22069787650957, 'Persona Coverage': 3.4548921443495106, 'Persona Recall': 0.0, 'Persona Precision': 3.3430187212193525, 'Persona F1': 0.0}\n"
     ]
    }
   ],
   "source": [
    "from analyze import print_avg_metrics\n",
    "print_avg_metrics(\"experiment1_metrics\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
